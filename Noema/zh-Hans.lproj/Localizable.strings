/* Auto-generated localization file. */
"%@" = "%@";
"%@ %" = "%1$@ %2$";
"%@ %@" = "%1$@ %2$@";
"%@ models" = "%@ 型号";
"%@ of %@ budget" = "%2$@ 预算中的 %1$@";
"%@ tokens" = "%@ 代币";
"%@ – %@" = "%1$@ – %2$@";
"%@ • %@" = "%1$@ • %2$@";
"%@%" = "%1$@%2$";
"%@% · %@" = "%1$@%2$ · %3$@";
"%@." = "%@。";
"%@:" = "%@：";
"(+ mmproj %@%)" = "(+ mmproj %1$@%2$)";
"... and %@ more" = "...以及 %@ 更多";
"320 MB • One-time download" = "320 MB • 一次性下载";
"320 MB • One‑time download used for local dataset search" = "320 MB • 一次性下载用于本地数据集搜索";
"A larger context keeps more conversation history, but also uses more memory. Adjust it here." = "更大的上下文可以保留更多的对话历史记录，但也会使用更多的内存。在这里调整一下。";
"Active" = "积极的";
"Active Model" = "主动模型";
"Active connection via %@" = "通过 %@ 进行活动连接";
"Active experts per token: %@ of %@" = "每个代币的活跃专家：%1$@ / %2$@";
"Add a remote backend to configure remote startup fallbacks." = "添加远程后端以配置远程启动回退。";
"Add one or two datasets (like open textbooks) to keep responses accurate and help the AI cite sources." = "添加一两个数据集（例如开放教科书）以保持回复准确并帮助人工智能引用来源。";
"Add remote endpoint" = "添加远程端点";
"Adjust appearance, privacy options, and network preferences here." = "在此处调整外观、隐私选项和网络首选项。";
"Advanced" = "先进的";
"Advanced Controls" = "高级控制";
"Allows models to use a privacy-preserving web search API when you tap the globe in chat. Default is ON. In Offline Only mode, the button is disabled." = "当您在聊天中点击地球仪时，允许模型使用保护隐私的网络搜索 API。默认为开。在仅离线模式下，该按钮被禁用。";
"Analyzing context..." = "正在分析上下文...";
"App Memory Usage (estimated)" = "应用程序内存使用情况（估计）";
"App memory usage budget: %@ (conservative)" = "应用程序内存使用预算：%@（保守）";
"Approx. Tokens" = "大约。代币";
"Arm web search when you truly need outside info. It has a small daily limit and most chats don’t require it." = "当您真正需要外部信息时，请启动网络搜索。它的每日限制很小，大多数聊天都不需要它。";
"Ask Noema anything" = "向诺玛询问任何事情";
"Ask…" = "问…";
"Attach Photos" = "附上照片";
"Backend not found" = "找不到后端";
"Benchmark running…" = "基准运行...";
"Benchmarking is not available for this model format." = "基准测试不适用于此模型格式。";
"Blocks all network traffic, model downloads, and cloud connections so everything stays on‑device." = "阻止所有网络流量、模型下载和云连接，以便所有内容都保留在设备上。";
"Bluetooth Pairing" = "蓝牙配对";
"Browse community models and curated datasets to expand what Noema can do." = "浏览社区模型和精选数据集以扩展 Noema 的功能。";
"Browse curated datasets for retrieval" = "浏览精选数据集以进行检索";
"CFBundleShortVersionString1.0" = "CFBundleShortVersionString1.0";
"Cancel" = "取消";
"Cancel Benchmark" = "取消基准测试";
"Discover Intelligence" = "探索智能";
"Catalog" = "目录";
"Chat" = "聊天";
"Chat privately with your local models, sync datasets, and manage the relay server in one place." = "与本地模型私下聊天、同步数据集并在一处管理中继服务器。";
"Chats" = "聊天记录";
"Checking…" = "检查…";
"Citation %@" = "引文 %@";
"Cloud Relay Container" = "云中继容器";
"Cloud Relay via CloudKit (auto-discovery and Bluetooth pairing)" = "通过 CloudKit 的云中继（自动发现和蓝牙配对）";
"CloudKit" = "云套件";
"CloudKit bridge active. Local replies are generated on this Mac." = "CloudKit 桥接器处于活动状态。本地回复是在此 Mac 上生成的。";
"Compiling Metal kernels for GGUF models can take up to a minute on first load." = "首次加载时，为 GGUF 模型编译 Metal 内核最多可能需要一分钟时间。";
"Complete the streaming response in the active chat before sending again." = "在再次发送之前，请在活动聊天中完成流式响应。";
"Confirm and Start Embedding" = "确认并开始嵌入";
"Connected" = "已连接";
"Connection Modes" = "连接方式";
"Connection Status: %@" = "连接状态：%@";
"Container" = "容器";
"Context Length" = "上下文长度";
"Context Length: 4096 tokens" = "上下文长度：4096 个标记";
"Context length is under 5000 tokens. With images and multi-sequence decoding (n_seq_max=16), per-sequence memory can be too small, leading to a crash. Increase context to at least 8192 in Model Settings." = "上下文长度低于 5000 个标记。对于图像和多序列解码（n_seq_max=16），每个序列的内存可能太小，从而导致崩溃。在模型设置中将上下文增加到至少 8192。";
"Controls how many high‑scoring passages (chunks) can be injected into the prompt. Higher values increase recall but consume more context window and can slow responses. Typical range 3–6." = "控制可以将多少高分段落（块）注入到提示中。较高的值会增加召回率，但会消耗更多的上下文窗口并可能减慢响应速度。典型范围 3–6。";
"Couldn't load the recommended model." = "无法加载推荐模型。";
"Couldn’t load the recommended model right now." = "目前无法加载推荐模型。";
"Creativity: %@. Low values focus responses; high values add variety." = "创造力：%@。低值焦点响应；高价值增加了​​多样性。";
"Dark" = "黑暗的";
"Dataset" = "数据集";
"Dataset indexing in progress..." = "数据集索引正在进行中...";
"Dataset ready to use" = "数据集可供使用";
"Datasets" = "数据集";
"Datasets enrich the model with focused knowledge. Toggle one on to use it in chat." = "数据集通过集中的知识丰富了模型。将其打开即可在聊天中使用它。";
"Default selection (~%@) balances RAM usage against model quality." = "默认选择 (~%@) 平衡 RAM 使用与模型质量。";
"Delete" = "删除";
"Delete Dataset" = "删除数据集";
"Deletes all chats, downloaded models, and datasets, and restores settings to defaults. The embedding model stays installed." = "删除所有聊天、下载的模型和数据集，并将设置恢复为默认值。嵌入模型保持安装状态。";
"Device" = "设备";
"Digest:" = "消化：";
"Done" = "完毕";
"Done!" = "完毕！";
"Download Now" = "立即下载";
"Download a model from Explore or add a remote endpoint to get started." = "从 Explore 下载模型或添加远程端点以开始使用。";
"Download a small embedding model so Noema can index and search your datasets" = "下载一个小型嵌入模型，以便 Noema 可以索引和搜索您的数据集";
"Downloaded datasets need on-device embedding. Give it a few minutes after download finishes." = "下载的数据集需要在设备上嵌入。下载完成后请等待几分钟。";
"Downloaded models and datasets live here so you can manage them offline." = "下载的模型和数据集位于此处，以便您可以离线管理它们。";
"Downloading…" = "正在下载...";
"Draft tokens: %@" = "草稿代币：%@";
"Draft window: %@" = "草稿窗口：%@";
"EPUB viewing not supported on this platform" = "此平台不支持 EPUB 观看";
"Embedding" = "嵌入";
"Embedding Model Ready" = "嵌入模型就绪";
"Embedding is resource intensive. For best performance, plug in your phone. Do you want to proceed on battery?" = "嵌入是资源密集型的。为了获得最佳性能，请插入手机。您想继续使用电池吗？";
"Enabling Bluetooth…" = "启用蓝牙...";
"Enhance with Datasets" = "使用数据集增强";
"Error" = "错误";
"Estimated working set: %@ · Budget: %@" = "预计工作集：%1$@ · 预算：%2$@";
"Experts Per Token" = "每个代币的专家";
"Explore Datasets" = "探索数据集";
"Expose any downloaded models or connected remote endpoints from the Stored tab to your paired devices. Select which one should answer conversations when the relay is running." = "将“存储”选项卡中所有下载的模型或连接的远程端点公开到配对设备。选择中继运行时应由哪一个应答对话。";
"Expose to iOS" = "暴露给 iOS";
"Explore the latest open-source models optimized for your Mac." = "探索为您的 Mac 优化的最新开源模型。";
"Failed to load README" = "无法加载自述文件";
"Failed: %@" = "失败：%@";
"Fastest option on this device: SLM (Leap) models." = "此设备上最快的选项：SLM (Leap) 型号。";
"Field requirements will depend on your specific backend deployment." = "现场要求将取决于您的具体后端部署。";
"Files" = "文件";
"First, enable fast dataset search" = "首先，启用快速数据集搜索";
"First-time GGUF load takes longer" = "首次 GGUF 加载需要更长的时间";
"First-time download from HuggingFace" = "首次从 HuggingFace 下载";
"First‑time setup: download the Qwen‑1.7B model and embeddings.\nWi‑Fi recommended." = "首次设置：下载 Qwen-1.7B 模型和嵌入。\n建议使用 Wi-Fi。";
"For best performance, please plug in your phone until this completes." = "为了获得最佳性能，请插入手机直至完成。";
"Force Local Network" = "强制本地网络";
"Forces chat traffic through the last LAN host even if Wi‑Fi names don't match yet." = "即使 Wi-Fi 名称尚不匹配，也会强制聊天流量通过最后一个 LAN 主机。";
"Formatted view unavailable" = "格式化视图不可用";
"Found unsupported: %@ …" = "发现不受支持：%@ ...";
"Frequency penalty: %@" = "频率损失：%@";
"GGUF models are the most compatible option. Use the format switch to explore the other builds when you need them." = "GGUF 模型是最兼容的选择。当您需要时，使用格式开关探索其他版本。";
"GGUF works everywhere. MLX targets Apple Silicon speed. SLM focuses on responsiveness on any device." = "GGUF 无处不在。 MLX 的目标是 Apple Silicon 的速度。 SLM 专注于任何设备上的响应能力。";
"GPU Offload Layers" = "GPU 卸载层";
"GPU off-load is not supported for this model." = "此型号不支持 GPU 卸载。";
"Get Started" = "开始使用";
"Help shape Noema by trying upcoming features and sharing feedback." = "通过尝试即将推出的功能和分享反馈来帮助塑造 Noema。";
"High context lengths use more memory" = "高上下文长度使用更多内存";
"High-quality embedding model for local RAG" = "本地 RAG 的高质量嵌入模型";
"Host ID: %@" = "主机 ID：%@";
"How it works" = "它是如何运作的";
"I'm New to Local LLMs, Guide Me" = "我是本地法学硕士的新手，请指导我";
"If enabled, the app will attempt to load models even when they likely exceed your device's memory budget. This can cause the app to terminate." = "如果启用，应用程序将尝试加载模型，即使它们可能超出设备的内存预算。这可能会导致应用程序终止。";
"Import Dataset" = "导入数据集";
"Import PDFs, EPUBs, or text files to build local knowledge bases." = "导入 PDF、EPUB 或文本文件以构建本地知识库。";
"Import your own PDFs, EPUBs, or TXT files and keep them local." = "导入您自己的 PDF、EPUB 或 TXT 文件并将其保存在本地。";
"Importing & Scanning..." = "导入和扫描...";
"In progress..." = "进行中...";
"Indexing %@" = "索引 %@";
"Indexing dataset…" = "索引数据集...";
"Indexing: %@% · %@" = "索引： %1$@%2$ · %3$@";
"Install a local model to make it available at launch." = "安装本地模型以使其在启动时可用。";
"Install another model with the same architecture and equal or smaller size to enable speculative decoding." = "安装具有相同架构和相同或更小尺寸的另一个模型以启用推测解码。";
"K Cache Quant" = "K 缓存数量";
"Keep this iPhone or iPad within a few feet of the Mac that is advertising Noema Relay. We'll pull the relay details automatically once connected." = "将此 iPhone 或 iPad 放置在距离宣传 Noema Relay 的 Mac 几英尺范围内。连接后，我们将自动提取中继详细信息。";
"LAN URL: %@" = "局域网网址：%@";
"Large Model Downloads" = "大型模型下载";
"Last Sync" = "上次同步";
"Last refreshed %@" = "最后刷新 %@";
"Latest benchmark" = "最新基准";
"Latest integrated release: %@" = "最新集成版本：%@";
"Library" = "图书馆";
"Light" = "光";
"Llama.cpp" = "调用.cpp";
"Load" = "加载";
"Load a local model before chatting. You can download one from the Explore tab or load a model you've already installed." = "在聊天之前加载本地模型。您可以从“探索”选项卡下载一个模型或加载已安装的模型。";
"Loading recommendation…" = "正在加载推荐...";
"Loading…" = "加载中…";
"Local Network HTTP server for LAN clients (OpenAI-compatible)" = "LAN 客户端的本地网络 HTTP 服务器（兼容 OpenAI）";
"Low = focused. High = varied." = "低=专注。高=多样化。";
"Lower = more results (more noise). Higher = stricter matches." = "更低=更多结果（更多噪音）。更高=更严格的匹配。";
"MLX currently manages expert routing automatically; manual selection is not supported." = "MLX 目前自动管理专家路由；不支持手动选择。";
"Many models are several gigabytes in size and require a stable connection and sufficient storage. Downloads can fail or take a long time on slow networks or devices with limited space." = "许多型号的大小为数千兆字节，需要稳定的连接和足够的存储空间。在慢速网络或空间有限的设备上，下载可能会失败或需要很长时间。";
"Max Chunks: %@" = "最大块数：%@";
"Max recommended context on this device: ~%@ tokens" = "此设备上推荐的最大上下文：~%@ 令牌";
"Measure real-world generation speed for this configuration. A short scripted prompt will run locally and report timing and memory usage." = "测量此配置的实际生成速度。一个简短的脚本提示将在本地运行并报告时间和内存使用情况。";
"Min-p" = "最小p";
"Min-p: %@" = "最小 p: %@";
"Minimum cosine similarity a passage must have to be considered relevant. Lower = more passages (higher recall, more noise). Higher = fewer, more precise passages. Try 0.2–0.4 for broad questions; 0.5–0.7 for precise lookups." = "必须将段落的最小余弦相似度视为相关。较低=更多段落（更高的召回率，更多的噪音）。更高=更少、更精确的段落。对于广泛的问题，尝试使用 0.2–0.4； 0.5–0.7 用于精确查找。";
"MoE layers: %@ / %@" = "MoE 层：%1$@ / %2$@";
"Model Detection Limitations" = "模型检测的局限性";
"Model Formats" = "模型格式";
"Models" = "型号";
"Models shown here are exposed by the Mac relay. Manage sources in the Relay tab on macOS to share more models." = "此处显示的模型由 Mac 继电器公开。在 macOS 上的“中继”选项卡中管理源以共享更多模型。";
"Move your device closer to the Mac running the relay if it doesn't appear right away. Bluetooth discovery usually completes within a few seconds." = "如果没有立即出现，请将您的设备移近运行中继的 Mac。蓝牙发现通常在几秒钟内完成。";
"Name your dataset" = "为您的数据集命名";
"Nearby Relays" = "附近的继电器";
"Nearby iPhone and iPad devices discover your Mac relay instantly and sync pairing codes over the air." = "附近的 iPhone 和 iPad 设备会立即发现您的 Mac 中继并通过无线方式同步配对码。";
"Need a fresh thread? Tap the plus button for a brand-new conversation." = "需要新线程吗？点击加号按钮即可进行全新对话。";
"Nice! You already have the recommended GGUF starter model ready to use." = "好的！您已经准备好可以使用推荐的 GGUF 入门模型。";
"No compatible files found for retrieval. Supported: PDF, EPUB, TXT, MD, JSON, JSONL, CSV, TSV" = "找不到可供检索的兼容文件。支持：PDF、EPUB、TXT、MD、JSON、JSONL、CSV、TSV";
"No connection responses recorded yet." = "尚未记录连接响应。";
"No datasets available yet. Import or download datasets to build your personal library." = "尚无可用数据集。导入或下载数据集以构建您的个人图书馆。";
"No datasets found. Try different keywords." = "未找到数据集。尝试不同的关键词。";
"No datasets imported yet." = "尚未导入数据集。";
"No datasets yet" = "还没有数据集";
"No files listed for this dataset." = "没有为此数据集列出文件。";
"No model >" = "暂无型号 >";
"No model loaded" = "未加载模型";
"No models available. Add downloads or remote connections in Stored to configure the relay." = "无可用型号。在“已存储”中添加下载或远程连接以配置中继。";
"No models cached yet. Open the backend to refresh its catalog." = "尚未缓存任何模型。打开后端刷新其目录。";
"No models found for '%@'" = "未找到“%@”的型号";
"No models loaded right now. We'll spin one up when a request arrives." = "目前没有加载模型。当请求到达时，我们会启动一个。";
"No models match your search." = "没有符合您搜索条件的型号。";
"No models yet" = "还没有型号";
"No parameters" = "无参数";
"No quant files available" = "没有可用的定量文件";
"No recent devices. We'll list clients the next time they talk to this relay." = "没有最近的设备。我们将在客户下次与此中继通信时列出客户。";
"No remote endpoints configured yet." = "尚未配置远程端点。";
"No remote endpoints configured." = "未配置远程端点。";
"No ≥Q3 quants are available for this model." = "此模型没有可用的 ≥Q3 定量。";
"Noema" = "十一月";
"Noema REST API — /api/v0/* for model catalog & operations" = "Noema REST API — /api/v0/* 用于模型目录和操作";
"Noema Relay" = "诺埃玛继电器";
"Noema Server" = "诺玛服务器";
"Noema attempts to gauge available memory to prevent models from exceeding device limits. These checks may occasionally miss risky situations and allow a model to crash your app, or they may be overly conservative and block a model that could have run fine." = "Noema 尝试测量可用内存以防止模型超出设备限制。这些检查有时可能会错过风险情况并允许模型使您的应用程序崩溃，或者它们可能过于保守并阻止本来可以正常运行的模型。";
"Noema could not find a projector in the repository. If the model advertises vision, ensure the mmproj file is present in the same folder as the weights." = "Noema 在存储库中找不到投影仪。如果模型宣传视觉，请确保 mmproj 文件与权重位于同一文件夹中。";
"Noema has been reset. The embedding model remains installed." = "诺玛已重置。嵌入模型仍保持安装状态。";
"Nomic Embed Text v1.5 (Q4_K_M)" = "Nomic 嵌入文本 v1.5 (Q4_K_M)";
"None" = "没有任何";
"Not found" = "未找到";
"Not provided" = "未提供";
"OK" = "好的";
"Off-Grid" = "离网";
"Off-grid mode blocks every network call so the app stays self-contained. Good luck exploring Noema!" = "离网模式会阻止每个网络调用，因此应用程序保持独立。祝您探索 Noema 好运！";
"Only one expert is available for this model; the active expert count is fixed." = "该模型只有一名专家；活跃专家数量是固定的。";
"Open Stored to choose a model to run locally or connect to a remote endpoint." = "打开“存储”以选择要在本地运行或连接到远程端点的模型。";
"Open the sidebar to revisit any previous session without losing your spot." = "打开侧边栏即可重新访问之前的任何会话，而不会丢失您的位置。";
"OpenAI-style API — /v1/chat/completions, /v1/completions, /v1/models" = "OpenAI 风格的 API — /v1/chat/completions、/v1/completions、/v1/models";
"Optimizations in use" = "使用中的优化";
"PDF viewing not supported on this platform" = "此平台不支持 PDF 查看";
"Pick a model and add a dataset" = "选择模型并添加数据集";
"Pick the SLM format when you want ultra-responsive models that run well anywhere." = "当您想要在任何地方都能良好运行的超响应模型时，请选择 SLM 格式。";
"Pinned answer" = "置顶答案";
"Pinned answer unavailable" = "固定答案不可用";
"Preparation" = "准备";
"Preparing Embedding Model" = "准备嵌入模型";
"Preparing benchmark…" = "准备基准...";
"Preparing…" = "正在准备……";
"Presence penalty: %@" = "存在惩罚：%@";
"Projector (mmproj)" = "投影仪（mmproj）";
"Projector downloaded automatically from Hugging Face. Keep this file alongside the weights so vision remains available." = "投影仪从 Hugging Face 自动下载。将此文件与权重放在一起，以便视力保持可用。";
"Quantize the runtime key cache to save memory. Experimental." = "量化运行时密钥缓存以节省内存。实验性的。";
"Quantize the runtime value cache to save memory when Flash Attention is enabled. Experimental." = "启用 Flash Attention 时量化运行时值缓存以节省内存。实验性的。";
"Qwen 3 1.7B GGUF (Q3_K_M) gives you a dependable starting point. Delete it anytime if you need space." = "Qwen 3 1.7B GGUF (Q3_K_M) 为您提供可靠的起点。如果您需要空间，请随时删除它。";
"RAG embeds normalized paragraphs from your PDFs and EPUBs. On each question, the most relevant chunks are retrieved and added to the prompt. Images are ignored." = "RAG 嵌入 PDF 和 EPUB 中的规范化段落。对于每个问题，都会检索最相关的块并将其添加到提示中。图像被忽略。";
"RAM Safety Checks" = "RAM 安全检查";
"RAM information for this device will be added in a future update." = "该设备的 RAM 信息将在未来的更新中添加。";
"REST Endpoints" = "休息端点";
"Reachable at" = "可到达：";
"Ready for Use" = "准备使用";
"Recent Chats" = "最近的聊天记录";
"Recommended" = "受到推崇的";
"Recommended Starter Model" = "推荐入门型号";
"Relay ID" = "中继ID";
"Relay Server Running" = "中继服务器运行";
"Relay Sources" = "中继源";
"Remaining: %@" = "剩余：%@";
"Remember:" = "记住：";
"Remote Backends" = "远程后端";
"Remote endpoint is offline. This model can't be found at this time." = "远程端点离线。目前无法找到该型号。";
"Remote timeout: %@s" = "远程超时：%@s";
"Repeat last N tokens: %@" = "重复最后 N 个标记：%@";
"Repetition penalty: %@" = "重复罚分：%@";
"Request Parameters" = "请求参数";
"Responses are generated by the macOS relay server. Configure the provider (LM Studio or Ollama) on the Mac app." = "响应由 macOS 中继服务器生成。在 Mac 应用程序上配置提供程序（LM Studio 或 Ollama）。";
"Result" = "结果";
"Results" = "结果";
"Save" = "节省";
"Saving…" = "保存…";
"Score: %@" = "分数：%@";
"SearXNG web search is available without limits. There's nothing to purchase—just enable the globe button in chat whenever you need online results." = "SearXNG 网络搜索不受限制。无需购买任何东西，只需在需要在线结果时启用聊天中的地球按钮即可。";
"SearXNG web search is enabled for this device." = "此设备已启用 SearXNG Web 搜索。";
"Search" = "搜索";
"Search for any subject you're interested in." = "搜索您感兴趣的任何主题。";
"Search requests are proxied through https://search.noemaai.com and are available without quotas." = "搜索请求通过 https://search.noemaai.com 进行代理，并且无需配额即可使用。";
"Seed" = "种子";
"Selecting more experts keeps additional expert weights resident in RAM and increases memory usage." = "选择更多专家可以使额外的专家权重驻留在 RAM 中并增加内存使用量。";
"Server Settings" = "服务器设置";
"Share Logs" = "分享日志";
"Sharing relay payload with nearby devices…" = "与附近的设备共享中继有效负载...";
"Shows the last server response." = "显示最后的服务器响应。";
"Signal" = "信号";
"Similarity Threshold" = "相似度阈值";
"Simple" = "简单的";
"Smooth loops and phrase echo by balancing repetition controls." = "通过平衡重复控制来平滑循环和短语回声。";
"Smooth loops and repeated phrases by tuning repetition controls." = "通过调整重复控制来平滑循环和重复的短语。";
"Some models do not provide the system prompts needed for Noema to detect and configure them properly. These models may be unusable until they include appropriate metadata or support." = "某些型号不提供 Noema 正确检测和配置它们所需的系统提示。这些模型在包​​含适当的元数据或支持之前可能无法使用。";
"Source unavailable. Check storage or network settings." = "来源不可用。检查存储或网络设置。";
"Source: %@" = "来源：%@";
"Specify identifiers for models that are not listed by the server. Leave blank to rely on the server's catalog." = "指定服务器未列出的模型的标识符。留空以依赖服务器的目录。";
"Specify your model identifiers, or reload your custom models later." = "指定您的模型标识符，或稍后重新加载您的自定义模型。";
"Speed up with a smaller helper model." = "使用较小的辅助模型来加快速度。";
"Start by installing one model. Then add a dataset (like an open textbook) so the AI can answer with grounded knowledge." = "首先安装一个模型。然后添加一个数据集（如一本开放的教科书），以便人工智能可以用扎实的知识来回答。";
"Start the relay to automatically share the latest payload with nearby devices." = "启动中继以自动与附近的设备共享最新的有效负载。";
"Start with a reliable Qwen 3 1.7B build. It balances capability with small download size." = "从可靠的 Qwen 3 1.7B 版本开始。它平衡了功能与小下载大小。";
"Startup defaults now live in Settings → Startup. Favorite models here to keep them handy." = "启动默认设置现在位于“设置”→“启动”中。最喜欢的型号放在这里，方便携带。";
"Status" = "地位";
"Stay close to your Mac" = "靠近您的 Mac";
"Stop Using Dataset" = "停止使用数据集";
"Streaming" = "流媒体";
"Streaming response…" = "流媒体响应...";
"Supported Endpoints" = "支持的端点";
"Supported formats: PDF, EPUB, TXT, MD, JSON, JSONL, CSV, TSV" = "支持的格式：PDF、EPUB、TXT、MD、JSON、JSONL、CSV、TSV";
"Swap between the new stacked chat panel and the classic tab bar layout." = "在新的堆叠聊天面板和经典的选项卡栏布局之间进行交换。";
"Swipe left to remove the embedding model from this device." = "向左滑动即可从此设备中删除嵌入模型。";
"Switch the selector to MLX for Apple Silicon‑optimized builds that excel at speed." = "将选择器切换到 MLX，以获得速度出色的 Apple Silicon 优化版本。";
"Switch to Raw to inspect the original response." = "切换到原始以检查原始响应。";
"System" = "系统";
"Tap to load" = "点击加载";
"Temperature" = "温度";
"Testing" = "测试";
"The app memory usage budget is an estimate based on your device's total RAM and typical iOS memory management. The actual available memory may vary depending on system load, other running apps, and iOS memory pressure. Models that exceed this budget may cause the app to be terminated by iOS." = "应用程序内存使用预算是根据设备的总 RAM 和典型的 iOS 内存管理进行估算的。实际可用内存可能会因系统负载、其他正在运行的应用程序和 iOS 内存压力而有所不同。超过此预算的型号可能会导致应用程序被 iOS 终止。";
"The embedding model is installed. Delete it to free ~320 MB." = "嵌入模型已安装。删除它以释放 ~320 MB。";
"The relay listens to the %@ container for new conversations and responds with your selected provider." = "中继侦听 %@ 容器中的新对话，并与您选择的提供商进行响应。";
"The tool returned data that can't be formatted. Switch to Raw to inspect the original response." = "该工具返回了无法格式化的数据。切换到原始以检查原始响应。";
"These options stay in simple mode for clarity. Let’s cover the essentials." = "为了清晰起见，这些选项保持简单模式。让我们介绍一下要点。";
"Think of Noema as a simple way to run AI on your device. To get useful answers, you pair a local model with datasets (like open textbooks). We’ll guide you through the first setup." = "将 Noema 视为在设备上运行 AI 的简单方法。为了获得有用的答案，您可以将本地模型与数据集（如开放教科书）配对。我们将指导您完成第一个设置。";
"This app bundles llama.cpp; we keep this in sync with upstream b‑releases." = "该应用程序捆绑了 llama.cpp；我们将其与上游 b 版本保持同步。";
"This backend is unavailable. Remove it or pick another option." = "该后端不可用。删除它或选择其他选项。";
"This chat stays private—responses are generated on your device after you load a model." = "此聊天保持私密状态 - 加载模型后，系统会在您的设备上生成响应。";
"This configuration exceeds the current RAM safety guard, so benchmarking is disabled." = "此配置超出了当前 RAM 安全防护，因此基准测试被禁用。";
"This dataset is taking a while to load, still working…" = "该数据集需要一段时间才能加载，但仍在工作......";
"This dataset's files are not currently supported for document retrieval." = "目前不支持文档检索此数据集的文件。";
"This device doesn't support GPU offload." = "此设备不支持 GPU 卸载。";
"This device doesn't support GPU offload; GGUF models will run on the CPU and generation speed will be significantly slower." = "该设备不支持GPU卸载； GGUF 模型将在 CPU 上运行，生成速度会明显变慢。";
"This model doesn't support GPU offload and generation speed will be significantly slower. Consider switching to an MLX model." = "该模型不支持GPU卸载，生成速度会明显变慢。考虑切换到 MLX 模型。";
"This model doesn't support GPU offload and generation speed will be significantly slower. Fastest option on this device: use an SLM (Leap) model." = "该模型不支持GPU卸载，生成速度会明显变慢。此设备上最快的选项：使用 SLM (Leap) 模型。";
"This model doesn't support GPU offload and may run slowly. Consider an MLX model." = "此模型不支持 GPU 卸载，并且可能运行缓慢。考虑 MLX 模型。";
"This model doesn't support GPU offload and may run slowly. Fastest option: use an SLM model." = "此模型不支持 GPU 卸载，并且可能运行缓慢。最快的选择：使用 SLM 模型。";
"This permanently removes every chat conversation. This action cannot be undone." = "这将永久删除所有聊天对话。此操作无法撤消。";
"This textbook appears to be available only as a web page. Noema can't import it as a dataset." = "这本教科书似乎仅以网页形式提供。 Noema 无法将其作为数据集导入。";
"Tool calling isn't perfect. Although Noema implements many methods of detecting and instructing models to use tools, not all LLMs will follow instructions and some might not call them correctly or at all. Tool calling heavily depends on model pre-training and will get better as time passes." = "工具调用并不完美。尽管 Noema 实现了许多检测和指导模型使用工具的方法，但并非所有法学硕士都会遵循指令，有些可能无法正确调用或根本无法调用它们。工具调用在很大程度上取决于模型预训练，并且随着时间的推移会变得更好。";
"Tools" = "工具";
"Top-k: %@" = "前 k：%@";
"Top-p" = "顶p";
"Top-p: %@" = "顶部：%@";
"Try again" = "再试一次";
"Try another dataset if these formats aren't available." = "如果这些格式不可用，请尝试其他数据集。";
"Try the Qwen 3 1.7B GGUF (Q3_K_M) build below. It's a good starting point and you can delete it anytime." = "尝试下面的 Qwen 3 1.7B GGUF (Q3_K_M) 版本。这是一个很好的起点，您可以随时将其删除。";
"Unable to load image" = "无法加载图像";
"Unknown error" = "未知错误";
"Updates every second" = "每秒更新一次";
"Use" = "使用";
"Use Dataset" = "使用数据集";
"Use this switch to flip between finding models or datasets." = "使用此开关可在查找模型或数据集之间切换。";
"Using %@" = "使用 %@";
"Using %@ of %@ budget" = "使用 %2$@ 预算中的 %1$@";
"Using more than %@ significantly increases RAM usage." = "使用超过 %@ 会显着增加 RAM 使用量。";
"Using server catalog" = "使用服务器目录";
"V Cache Quant" = "V 缓存数量";
"Vendor recommendation: %@" = "供应商推荐：%@";
"Version" = "版本";
"Version %@" = "版本 %@";
"Version 1.4" = "1.4版本";
"Vision models require a companion projector (.mmproj). Noema will fetch it automatically the next time you download this model." = "视觉模型需要配套投影仪 (.mmproj)。 Noema 将在您下次下载此模型时自动获取它。";
"Wait for the response in your other chat to finish before sending a new message." = "等待其他聊天中的回复完成，然后再发送新消息。";
"Waiting for tool response…" = "等待工具响应...";
"We'll extract text and prepare embeddings. You can also start later from the dataset details." = "我们将提取文本并准备嵌入。您也可以稍后从数据集详细信息开始。";
"We'll route new conversations through %@ even if Wi‑Fi names differ. You can switch back by reloading the backend." = "即使 Wi-Fi 名称不同，我们也会通过 %@ 路由新对话。您可以通过重新加载后端来切换回来。";
"We'll try remote models in priority order for this long before moving to the next option." = "在转向下一个选项之前，我们将按优先顺序尝试远程模型这么长时间。";
"We'll try this saved identifier even though it's not in the latest catalog." = "我们将尝试这个保存的标识符，即使它不在最新的目录中。";
"Web Search Tool Calls" = "网络搜索工具调用";
"Web Search button" = "网页搜索按钮";
"Web search is included" = "包括网络搜索";
"Weights" = "重量";
"Welcome to Noema" = "欢迎来到诺玛";
"Welcome to Noema for Mac" = "欢迎使用 Mac 版 Noema";
"What is Noema?" = "什么是Noema？";
"When connecting from another device, point the base URL to your computer (for example http://192.168.0.10:11434) and start Ollama with `OLLAMA_HOST=0.0.0.0` so it accepts remote clients." = "从其他设备连接时，将基本 URL 指向您的计算机（例如 http://192.168.0.10:11434）并使用“OLLAMA_HOST=0.0.0.0”启动 Ollama，以便它接受远程客户端。";
"When enabled, pressing eject on iOS tells this Mac to unload the active relay model." = "启用后，在 iOS 上按下弹出按钮会告诉 Mac 卸载活动的继电器模型。";
"When enabled, you will be asked to choose parameters every time a model loads." = "启用后，每次加载模型时都会要求您选择参数。";
"Wi-Fi: %@" = "无线网络：%@";
"Working set estimate (%@): %@ @ %@ tokens" = "工作集估计 (%1$@)：%2$@ @ %3$@ 令牌";
"Write prompts, instructions, or notes here. Press return to add new lines." = "在此写下提示、说明或注释。按回车键添加新行。";
"You can keep chatting while indexing finishes" = "索引完成后您可以继续聊天";
"You can only favorite up to three models." = "您最多只能收藏三个模型。";
"You can restart this process in the dataset settings any time." = "您可以随时在数据集设置中重新启动此过程。";
"You're in Off-Grid mode. The Explore tab is hidden and all network features are disabled. You can only use downloaded models and datasets." = "您处于离网模式。 “探索”选项卡已隐藏，所有网络功能均已禁用。您只能使用下载的模型和数据集。";
"Your Datasets" = "您的数据集";
"Your Models" = "您的模特";
"Your private AI workspace" = "您的私人人工智能工作空间";
"You’re ready to explore. Download models, add datasets, and start chatting." = "您已准备好探索。下载模型、添加数据集并开始聊天。";
"minutes" = "分钟";
"•" = "";
"• Close other applications to free up RAM" = "• 关闭其他应用程序以释放 RAM";
"• Embedding happens locally on your device" = "• 嵌入发生在您的设备本地";
"• Larger datasets take exponentially more time" = "• 更大的数据集需要更多的时间";
"• You can pause and resume downloads if needed" = "• 如果需要，您可以暂停和恢复下载";
"…and %@ more parameter%@" = "...以及 %1$@ 更多参数 %2$@";
"⚠️ " = "";
"General" = "通用";

"Privacy" = "隐私";

"About" = "关于";

"About & Support" = "关于与支持";

"Network" = "网络";

"Embedding Model" = "嵌入模型";

"Retrieval" = "检索";

"Early Testers" = "早期测试者";

"Build Info" = "构建信息";

"Settings" = "设置";

"Language" = "语言";
"Startup" = "启动";
"Search models" = "搜索模型";
"SLM Models - Liquid AI" = "SLM Models - Liquid AI";
"Import" = "导入";
"Import GGUF" = "导入 GGUF";
"Import MLX" = "导入 MLX";
"Import Failed" = "导入失败";
"Switching between GGUF/MLX modes" = "在 GGUF/MLX 模式间切换";
"Switching between GGUF/SLM modes" = "在 GGUF/SLM 模式间切换";
"Vision-capable model" = "视觉模型";
"All" = "全部";
"Text" = "文本";
"Vision" = "视觉";
"Continue" = "继续";
"Try bullet" = "尝试：\n• 更换关键词（如用“gemma-3”替代“gemma 3”）\n• %@\n• 调整 文本/视觉 筛选\n• 检查搜索筛选条件";
"Select one or more .gguf files. For vision models, also select the projector .gguf (files containing ‘mmproj’ or ‘projector’). Files will be copied into your local model library." = "Select one or more .gguf files. For vision models, also select the projector .gguf (files containing ‘mmproj’ or ‘projector’). Files will be copied into your local model library.";
"Select the model folder that contains config.json and the weights (safetensors/npz). The entire folder will be imported into your local model library." = "Select the model folder that contains config.json and the weights (safetensors/npz). The entire folder will be imported into your local model library.";
"K Cache Quantization" = "K 缓存量化";
"V Cache Quantization" = "V 缓存量化";
"Favorite Limit Reached" = "已达收藏上限";
"Overview" = "概览";
"Sampling" = "采样";
"Speculative Decoding" = "推测解码";
"Benchmark" = "基准";
"Maintenance" = "维护";
"MLX" = "MLX";
"Tokenizer Path (tokenizer.json)" = "分词器路径（tokenizer.json）";
"Back" = "返回";
"Favorite Model" = "收藏的模型";
"Reset to Default Settings" = "重置为默认设置";
"Delete Model" = "删除模型";
"Delete %@?" = "删除 %@？";
"Not provided by repository" = "仓库未提供";
"Unknown (not checked yet)" = "未知（尚未检查）";
"Keep Model In Memory" = "将模型保留在内存中";
"GPU Offload Layers: %@/%@" = "GPU 卸载层：%1$@/%2$@";
"CPU Threads: %@" = "CPU 线程：%@";
"Offload KV Cache to GPU" = "将 KV 缓存卸载到 GPU";
"Use mmap()" = "使用 mmap()";
"Random" = "随机";
"Flash Attention" = "Flash Attention";
"1 expert" = "1 个专家";
"%@ experts" = "%@ 个专家";
"Helper Model" = "辅助模型";
"Draft strategy" = "草稿策略";
"Run Benchmark" = "运行基准测试";
"Benchmarking…" = "正在跑基准…";
"Leap SLM models manage runtime optimizations automatically." = "Leap SLM 模型会自动管理运行时优化。";
"This format doesn't expose tunable runtime optimizations." = "此格式不提供可调的运行时优化。";
"Token processing" = "提示处理速度";
"Token generation" = "生成速度";
"Total time" = "总时间";
"First token" = "首个标记";
"Peak memory" = "峰值内存";
"Output tokens" = "输出标记";
"End Guide" = "结束引导";
"Streaming benchmark output…" = "正在流式传输基准输出…";
"Streaming… %@ chunks (~%@ tok est.)" = "流式传输… %1$@ 块（约 %2$@ 标记估计）";
"Streaming… %d chunks (~%d tok est.)" = "流式传输… %d 块（约 %d 标记估计）";
"The selected model's weights could not be located." = "找不到所选模型的权重。";
"Failed to load model for benchmark: %@" = "加载基准测试模型失败：%@";
"Benchmark generation failed: %@" = "基准生成失败：%@";
"K Cache" = "K 缓存";
"V Cache" = "V 缓存";
"KV Offload" = "KV 卸载";
"On" = "开";
"Off" = "关";
"GPU" = "GPU";
"CPU" = "CPU";
"%.1f tok/s" = "%.1f tok/s";
"%.1fs" = "%.1fs";
"%.2fs" = "%.2fs";
"Move Up" = "上移";
"Move Down" = "下移";
"Remove" = "移除";
"Startup remote options" = "启动远程选项";
"No models cached yet. Open the backend to refresh its catalog." = "尚无缓存模型。打开后端以刷新目录。";
"We'll try this saved identifier even though it's not in the latest catalog." = "即使最新目录中没有，我们也会尝试这个已保存的标识符。";
"This backend is unavailable. Remove it or pick another option." = "此后端不可用。请移除或选择其他选项。";
"Backend removed" = "后端已移除";
"What is Max Chunks?" = "什么是最大块数？";
"What is Similarity Threshold?" = "什么是相似度阈值？";
"Approx. %@" = "约 %@";
"Advanced mode shows developer options and diagnostics." = "高级模式会显示开发者选项和诊断信息。";
"Simple mode hides advanced settings for a cleaner interface." = "简单模式会隐藏高级设置，使界面更简洁。";
"Hide advanced controls" = "隐藏高级控件";
"Show advanced controls" = "显示高级控件";
"Adjust model settings" = "调整模型设置";
"Model Settings" = "模型设置";
"SLM models are not supported on this platform." = "此平台不支持 SLM 模型。";
"Model likely exceeds memory budget. Lower context or choose a smaller quant." = "模型可能超出内存预算。请降低上下文或选择更小的量化。";
"Apple bundle models aren't supported on macOS yet." = "Apple 捆绑模型尚未在 macOS 上支持。";
"Estimate: %@\nBudget: %@\nContext length: %@ tokens\n\nThis is an estimate based on your device’s memory budget, context length (KV cache), and typical runtime overheads. Actual usage may vary." = "估算：%@\n预算：%@\n上下文长度：%@ 个 tokens\n\n这是基于设备内存预算、上下文长度（KV 缓存）和典型运行开销的估算值。实际使用可能有所不同。";
"Model likely fits in RAM" = "模型可能适合 RAM";
"Model may not fit in RAM" = "模型可能不适合 RAM";
"Fits in RAM (estimated)" = "适合 RAM（估算）";
"May not fit (estimated)" = "可能不适合（估算）";
"Please provide a backend name." = "请输入后端名称。";
"A backend with this name already exists." = "已存在同名后端。";
"Backend not found." = "未找到后端。";
"This Noema Relay device is already configured." = "此 Noema Relay 设备已配置。";
"OpenAI API" = "OpenAI API";
"LM Studio" = "LM Studio";
"Ollama" = "Ollama";
"Cloud Relay" = "Cloud Relay";
"Noema Relay" = "Noema Relay";
"Compatible with OpenAI-style /v1 endpoints" = "兼容 OpenAI 风格的 /v1 端点";
"Connect to LM Studio's REST server" = "连接到 LM Studio 的 REST 服务器";
"Target an Ollama host for chat and pulls" = "将聊天和拉取目标指向 Ollama 主机";
"Use Noema's Cloud Relay on macOS" = "在 macOS 上使用 Noema 的 Cloud Relay";
"Pair with your Mac over CloudKit" = "通过 CloudKit 与 Mac 配对";
"Please provide the CloudKit container identifier." = "请输入 CloudKit 容器标识符。";
"Please provide the host device ID from the Mac relay." = "请输入 Mac 中继的主机设备 ID。";
"Missing host device ID for Noema Relay." = "缺少 Noema Relay 的主机设备 ID。";
"Missing CloudKit container identifier." = "缺少 CloudKit 容器标识符。";
"Relay catalog unavailable." = "中继目录不可用。";
"Relay catalog is still syncing. Open the Mac relay, ensure it is signed into iCloud, then try again in a moment." = "中继目录仍在同步中。请在 Mac 上打开中继，确认已登录 iCloud，然后重试。";
"Terms of Use" = "使用条款";
"Privacy Policy" = "隐私政策";
"Contact Support" = "联系支持";
"Write a Review" = "撰写评价";
"Notes & Issues" = "备注与问题";
"Qwen3-1.7B is a compact and efficient model from the Qwen3 family, suitable for on-device usage with strong general capabilities." = "Qwen3-1.7B 是 Qwen3 系列中紧凑高效的模型，适合具备强大通用能力的本地部署。";
"Gemma 3n E2B is a lightweight instruction-tuned model from Google's Gemma family, optimized for efficient on-device conversations." = "Gemma 3n E2B 是 Google Gemma 系列的轻量级指令微调模型，为高效本地对话优化。";
"Gemma 3n E2B is an instruction-tuned variant of Google's Gemma family built for efficient reasoning on low-resource devices.\nAvailable in GGUF quants (Q3_K_M, Q4_K_M, Q6_K) and an MLX 4-bit build for Apple Silicon.\n" = "Gemma 3n E2B 是 Gemma 家族面向指令的变体，旨在资源受限设备上高效推理。\n提供 GGUF 量化（Q3_K_M、Q4_K_M、Q6_K）以及适用于 Apple Silicon 的 MLX 4 位构建。\n";
"Phi-4 Mini Reasoning is a lightweight model from the Phi-4 family, tuned for strong reasoning and efficiency across tasks." = "Phi-4 Mini Reasoning 是 Phi-4 系列的轻量模型，针对强推理和效率进行了调优。";
"Phi-4 Mini Reasoning — a compact model in Microsoft’s Phi-4 line designed for logical reasoning, problem solving, and instruction-following. \nDistributed in efficient GGUF quants (Q3_K_L, Q4_K_M, Q6_K) and an MLX 4-bit variant for Apple Silicon devices.\n" = "Phi-4 Mini Reasoning —— Microsoft Phi-4 系列的紧凑模型，为逻辑推理、问题解决和指令跟随而设计。\n提供高效的 GGUF 量化（Q3_K_L、Q4_K_M、Q6_K）以及 Apple Silicon 的 MLX 4 位版本。\n";
"Runtime Safety" = "运行时安全";
"Bypass RAM safety check (may cause crashes)" = "跳过 RAM 安全检查（可能导致崩溃）";
"Estimate for" = "估算对象";
"Off-grid Mode" = "离线模式";
"Delete All Chats" = "删除所有聊天";
"Reset App Data" = "重置应用数据";
"Max Chunks" = "最大块数";
"Delete Embedding Model" = "删除嵌入模型";
"Override the app language. Defaults to the device language on first launch." = "覆盖应用语言。首次启动时默认使用设备语言。";
"Swap between the new stacked chat panel and the classic tab bar layout." = "在新的堆叠聊天面板和经典标签栏布局之间切换。";
"Available Quantizations" = "可用量化";
"Sort quantizations" = "排序量化";
"Quant" = "量化";
"Size ↑" = "大小 ↑";
"Size ↓" = "大小 ↓";
"Model Library" = "模型库";
"Type to filter models…" = "输入以筛选模型…";
"No models match your search." = "没有与搜索匹配的模型。";
"Browse Explore tab" = "打开 Explore 选项卡";
"Manually choose parameters" = "手动选择参数";
"The base URL looks invalid. Please include the host (e.g. http://127.0.0.1:1234)." = "基础 URL 似乎无效。请包含主机（例如 http://127.0.0.1:1234）。";
"Could not build the remote endpoint URL." = "无法构建远程端点 URL。";
"The server returned an unexpected response." = "服务器返回了意外的响应。";
"Server responded with status code %d." = "服务器返回状态码 %d。";
"Server responded with status code %d: %@" = "服务器返回状态码 %1$d：%2$@";
"Failed to decode server response." = "无法解码服务器响应。";

"Explore" = "探索";
"Search datasets" = "搜索数据集";
"Download" = "下载";
"Offline" = "离线";
"Tap to load" = "点按加载";
"%d models" = "%d 个模型";
"Updated %@" = "%@更新";
"No models fetched yet" = "尚未获取模型";
"Auth" = "认证";
"Local Network" = "本地网络";
"Direct" = "直接";
"LAN" = "局域网";
"LAN · %@" = "局域网 · %@";
"Backend" = "后端";
"Base URL" = "基础 URL";
"Chat Path" = "聊天路径";
"Models Path" = "模型路径";
"Endpoint Type" = "端点类型";
"Endpoints" = "端点";
"Authentication" = "身份验证";
"Model Identifiers" = "模型标识符";
"Name" = "名称";
"Host device ID" = "主机设备 ID";
"Host Device ID" = "主机设备 ID";
"CloudKit container identifier" = "CloudKit 容器标识符";
"Field requirements will depend on your specific backend deployment." = "字段要求取决于您的后端部署。";
"Uses Noema Relay configuration" = "使用 Noema Relay 配置";
"Chat: %@\nModels: %@" = "聊天：%@\n模型：%@";
"Download Dataset" = "下载数据集";
"No files listed for this dataset." = "此数据集未列出任何文件。";
"This dataset's files are not currently supported for document retrieval." = "此数据集的文件目前不支持文档检索。";
"Supported formats: %@" = "支持的格式：%@";
"Try another dataset if these formats aren't available." = "如果这些格式不可用，请尝试其他数据集。";
"Found unsupported: %@ …" = "发现不支持的格式：%@ …";
"This textbook appears to be available only as a web page. Noema can't import it as a dataset." = "此教材似乎仅以网页形式提供。Noema 无法将其导入为数据集。";
"Download complete" = "下载完成";
"Downloading…" = "正在下载…";
"No compatible files found for retrieval. Supported formats: %@" = "未找到可用于检索的兼容文件。支持的格式：%@";
"No internet connection." = "没有互联网连接。";
"Request timed out. Please try again." = "请求超时。请重试。";
"Connection was lost. Please try again." = "连接已丢失。请重试。";
"Unexpected error: %@" = "意外错误：%@";

"Downloaded" = "已下载";
"Compressed Text" = "压缩文本";
"Small" = "小";
"Medium" = "中";
"Large" = "大";
"Very Large" = "特大";
"Extreme" = "极大";
"Under 10 MB" = "小于 10 MB";
"10–50 MB" = "10–50 MB";
"50–200 MB" = "50–200 MB";
"200–500 MB" = "200–500 MB";
"Over 500 MB" = "大于 500 MB";
"Estimated Embedding Time" = "预计嵌入时间";
"Peak RAM Usage" = "峰值内存占用";
"Dataset Size" = "数据集大小";
"Performance Note" = "性能提示";
"Recommendation" = "建议";
"Remember:" = "注意：";
"• Close other applications to free up RAM" = "• 关闭其他应用以释放内存";
"• Embedding happens locally on your device" = "• 嵌入在你的设备本地完成";
"• Larger datasets take exponentially more time" = "• 较大的数据集所需时间呈指数增长";
"• You can pause and resume downloads if needed" = "• 如有需要，你可以暂停并恢复下载";
"Dataset Requirements" = "数据集要求";
"Got it" = "知道了";
"Check Requirements" = "查看要求";
"< 1 minute" = "< 1 分钟";
"%d minutes" = "%d 分钟";
"This dataset should embed quickly with minimal resource usage. Perfect for testing and quick experiments." = "该数据集应能快速嵌入并占用极少资源。非常适合测试和快速实验。";
"This dataset is a reasonable size for most systems. Embedding should complete in a few minutes." = "该数据集的大小适合大多数系统。嵌入应在几分钟内完成。";
"This is a substantial dataset. Ensure you have adequate RAM and expect embedding to take 10–30 minutes." = "这是一个较大的数据集。确保有足够的内存，嵌入可能需要 10–30 分钟。";
"This is a very large dataset. Embedding may take 30–60 minutes and requires significant RAM." = "这是一个非常大的数据集。嵌入可能需要 30–60 分钟并且需要大量内存。";
"This is an extremely large dataset. Consider splitting it into smaller parts for better performance." = "这是一个极大的数据集。考虑将其拆分为更小的部分以获得更好性能。";
"Go ahead and download! This size works well on all systems." = "可以直接下载！这个大小在所有系统上都表现良好。";
"Recommended for most users. Make sure you have at least 4GB of free RAM." = "推荐给大多数用户。请确保至少有 4GB 可用内存。";
"Recommended only if you have 8GB+ RAM available. Close other applications before embedding." = "仅在你有 8GB 以上可用内存时推荐。嵌入前请关闭其他应用。";
"Recommended only for systems with 16GB+ RAM. Consider processing during off-hours." = "仅推荐给拥有 16GB 以上内存的系统。考虑在非工作时间处理。";
"Not recommended for typical systems. Consider finding a smaller version or subset of this dataset." = "不建议在普通系统上使用。考虑寻找更小的版本或子集。";
"Sample dataset" = "示例数据集";
"Ready" = "就绪";
"Open" = "打开";

"Model doesn't support GPU offload" = "该模型不支持 GPU 卸载";
"Loading model…" = "正在加载模型…";
"Select a model to load" = "选择要加载的模型";
"Please wait" = "请稍候";
"Models Library" = "模型库";
"Sort" = "排序";
"Recency" = "最新";
"Size" = "大小";
"Name" = "名称";
"Load Failed" = "加载失败";
"Don't show again" = "不再显示";

/* Noema Relay – pairing & dataset helpers */
"Model file missing (.gguf)" = "模型文件缺失（.gguf）";
"Model path missing" = "模型路径缺失";
"Imported Dataset" = "已导入的数据集";
"Dataset name" = "数据集名称";
"Keep this device near the Mac that's running Noema Relay to import its settings." = "要导入其设置，请将此设备靠近正在运行 Noema Relay 的 Mac。";
"Scanning for your Mac relay…" = "正在扫描你的 Mac 中继…";
"Ready to scan nearby relays" = "准备扫描附近的中继";
"Bluetooth access is required to pair with the Mac relay." = "要与 Mac 中继配对，需要蓝牙访问权限。";
"Stop Scanning" = "停止扫描";
"Start Scan" = "开始扫描";
"Connection verified. Relay details imported from this Mac." = "连接已验证。已从此 Mac 导入中继详细信息。";
"Signal strength unavailable" = "信号强度不可用";
"Very close" = "非常近";
"Nearby" = "附近";
"Within one room" = "同一房间内";
"Move closer for a stronger signal" = "靠近一些以获得更强的信号。";
"This Mac" = "此 Mac";

/* Accessibility announcements */
"Model loaded." = "模型已加载。";
"Prompt submitted." = "提示已提交。";
"Generating response…" = "正在生成回复…";
"Response generated." = "回复已生成。";

/* Tabs & accessibility labels */
"Stored" = "存储";
"Web Search" = "网页搜索";
"Open Stored" = "打开存储";
"Message input" = "消息输入";
"What is Web Search button?" = "什么是网页搜索按钮？";

/* Mac chat quick-load menu */
"Open Model Library" = "打开模型库";
"Favorites" = "收藏夹";
"Recent" = "最近";
