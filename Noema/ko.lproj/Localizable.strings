/* Auto-generated localization file. */
"%@" = "%@";
"%@ %" = "%1$@ %2$";
"%@ %@" = "%1$@ %2$@";
"%@ models" = "%@ 모델";
"%@ of %@ budget" = "%2$@ 예산 중 %1$@";
"%@ tokens" = "%@ 토큰";
"%@ – %@" = "%1$@ – %2$@";
"%@ • %@" = "%1$@ • %2$@";
"%@%" = "%1$@%2$";
"%@% · %@" = "%1$@%2$ · %3$@";
"%@." = "%@.";
"%@:" = "%@:";
"(+ mmproj %@%)" = "(+ mmproj %1$@%2$)";
"... and %@ more" = "... 그리고 %@개 더";
"320 MB • One-time download" = "320MB • 일회성 다운로드";
"320 MB • One‑time download used for local dataset search" = "320MB • 로컬 데이터 세트 검색에 사용되는 일회성 다운로드";
"A larger context keeps more conversation history, but also uses more memory. Adjust it here." = "컨텍스트가 클수록 더 많은 대화 기록이 유지되지만 더 많은 메모리를 사용합니다. 여기에서 조정하세요.";
"Active" = "활동적인";
"Active Model" = "활성 모델";
"Active connection via %@" = "%@을(를) 통한 활성 연결";
"Active experts per token: %@ of %@" = "토큰당 활성 전문가: %2$@ 중 %1$@";
"Add a remote backend to configure remote startup fallbacks." = "원격 시작 대체를 구성하려면 원격 백엔드를 추가하세요.";
"Add one or two datasets (like open textbooks) to keep responses accurate and help the AI cite sources." = "응답의 정확성을 유지하고 AI가 출처를 인용하는 데 도움이 되도록 하나 또는 두 개의 데이터세트(예: 공개 교과서)를 추가하세요.";
"Add remote endpoint" = "원격 엔드포인트 추가";
"Adjust appearance, privacy options, and network preferences here." = "여기에서 모양, 개인정보 보호 옵션, 네트워크 기본 설정을 조정하세요.";
"Advanced" = "고급의";
"Advanced Controls" = "고급 제어";
"Allows models to use a privacy-preserving web search API when you tap the globe in chat. Default is ON. In Offline Only mode, the button is disabled." = "채팅에서 지구본을 탭하면 모델이 개인 정보 보호 웹 검색 API를 사용할 수 있습니다. 기본값은 켜짐입니다. 오프라인 전용 모드에서는 버튼이 비활성화됩니다.";
"Analyzing context..." = "맥락 분석 중...";
"App Memory Usage (estimated)" = "앱 메모리 사용량(예상)";
"App memory usage budget: %@ (conservative)" = "앱 메모리 사용 예산: %@(보수적)";
"Approx. Tokens" = "대략. 토큰";
"Arm web search when you truly need outside info. It has a small daily limit and most chats don’t require it." = "외부 정보가 꼭 필요한 경우 웹 검색을 활용하세요. 일일 한도가 작으며 대부분의 채팅에서는 이를 요구하지 않습니다.";
"Ask Noema anything" = "노에마에게 무엇이든 물어보세요";
"Ask…" = "묻다…";
"Attach Photos" = "사진 첨부";
"Backend not found" = "백엔드를 찾을 수 없습니다.";
"Benchmark running…" = "벤치마크 실행 중…";
"Benchmarking is not available for this model format." = "이 모델 형식에는 벤치마킹을 사용할 수 없습니다.";
"Blocks all network traffic, model downloads, and cloud connections so everything stays on‑device." = "모든 네트워크 트래픽, 모델 다운로드, 클라우드 연결을 차단하여 모든 것이 기기에 유지되도록 합니다.";
"Bluetooth Pairing" = "블루투스 페어링";
"Browse community models and curated datasets to expand what Noema can do." = "Noema가 할 수 있는 작업을 확장하려면 커뮤니티 모델과 엄선된 데이터세트를 찾아보세요.";
"Browse curated datasets for retrieval" = "검색을 위해 선별된 데이터 세트 찾아보기";
"CFBundleShortVersionString1.0" = "CFBundleShortVersionString1.0";
"Cancel" = "취소";
"Cancel Benchmark" = "벤치마크 취소";
"Discover Intelligence" = "인텔리전스를 발견하세요";
"Catalog" = "목록";
"Chat" = "채팅";
"Chat privately with your local models, sync datasets, and manage the relay server in one place." = "로컬 모델과 비공개로 채팅하고, 데이터 세트를 동기화하고, 릴레이 서버를 한 곳에서 관리하세요.";
"Chats" = "채팅";
"Checking…" = "확인 중…";
"Citation %@" = "인용 %@";
"Cloud Relay Container" = "클라우드 릴레이 컨테이너";
"Cloud Relay via CloudKit (auto-discovery and Bluetooth pairing)" = "CloudKit을 통한 Cloud Relay(자동 검색 및 Bluetooth 페어링)";
"CloudKit" = "CloudKit";
"CloudKit bridge active. Local replies are generated on this Mac." = "CloudKit 브리지가 활성화되었습니다. 로컬 응답은 이 Mac에서 생성됩니다.";
"Compiling Metal kernels for GGUF models can take up to a minute on first load." = "GGUF 모델용 Metal 커널을 컴파일하는 데는 첫 번째 로드 시 최대 1분이 걸릴 수 있습니다.";
"Complete the streaming response in the active chat before sending again." = "다시 보내기 전에 활성 채팅에서 스트리밍 응답을 완료하세요.";
"Confirm and Start Embedding" = "확인 및 삽입 시작";
"Connected" = "연결됨";
"Connection Modes" = "연결 모드";
"Connection Status: %@" = "연결 상태: %@";
"Container" = "컨테이너";
"Context Length" = "컨텍스트 길이";
"Context Length: 4096 tokens" = "컨텍스트 길이: 4096개 토큰";
"Context length is under 5000 tokens. With images and multi-sequence decoding (n_seq_max=16), per-sequence memory can be too small, leading to a crash. Increase context to at least 8192 in Model Settings." = "컨텍스트 길이는 토큰 5,000개 미만입니다. 이미지 및 다중 시퀀스 디코딩(n_seq_max=16)을 사용하면 시퀀스당 메모리가 너무 작아서 충돌이 발생할 수 있습니다. 모델 설정에서 컨텍스트를 8192 이상으로 늘립니다.";
"Controls how many high‑scoring passages (chunks) can be injected into the prompt. Higher values increase recall but consume more context window and can slow responses. Typical range 3–6." = "프롬프트에 얼마나 많은 점수를 받은 구절(청크)을 삽입할 수 있는지 제어합니다. 값이 높을수록 재현율은 높아지지만 더 많은 컨텍스트 창을 사용하고 응답이 느려질 수 있습니다. 일반적인 범위는 3~6입니다.";
"Couldn't load the recommended model." = "추천 모델을 로드할 수 없습니다.";
"Couldn’t load the recommended model right now." = "지금은 추천 모델을 로드할 수 없습니다.";
"Creativity: %@. Low values focus responses; high values add variety." = "창의성: %@. 낮은 값은 응답에 초점을 맞춥니다. 높은 가치는 다양성을 더해줍니다.";
"Dark" = "어두운";
"Dataset" = "데이터세트";
"Dataset indexing in progress..." = "데이터 세트 색인 생성 중...";
"Dataset ready to use" = "사용할 준비가 된 데이터 세트";
"Datasets" = "데이터세트";
"Datasets enrich the model with focused knowledge. Toggle one on to use it in chat." = "데이터 세트는 집중된 지식으로 모델을 강화합니다. 채팅에서 사용하려면 하나를 켜세요.";
"Default selection (~%@) balances RAM usage against model quality." = "기본 선택(~%@)은 모델 품질과 RAM 사용량의 균형을 맞춥니다.";
"Delete" = "삭제";
"Delete Dataset" = "데이터세트 삭제";
"Deletes all chats, downloaded models, and datasets, and restores settings to defaults. The embedding model stays installed." = "모든 채팅, 다운로드한 모델, 데이터 세트를 삭제하고 설정을 기본값으로 복원합니다. 임베딩 모델은 설치된 상태로 유지됩니다.";
"Device" = "장치";
"Digest:" = "요람:";
"Done" = "완료";
"Done!" = "완료!";
"Download Now" = "지금 다운로드";
"Download a model from Explore or add a remote endpoint to get started." = "시작하려면 탐색에서 모델을 다운로드하거나 원격 엔드포인트를 추가하세요.";
"Download a small embedding model so Noema can index and search your datasets" = "Noema가 데이터 세트를 색인화하고 검색할 수 있도록 작은 임베딩 모델을 다운로드하세요.";
"Downloaded datasets need on-device embedding. Give it a few minutes after download finishes." = "다운로드한 데이터세트에는 기기 내 삽입이 필요합니다. 다운로드가 완료된 후 몇 분 정도 기다리십시오.";
"Downloaded models and datasets live here so you can manage them offline." = "다운로드한 모델과 데이터세트가 여기에 있으므로 오프라인으로 관리할 수 있습니다.";
"Downloading…" = "다운로드 중…";
"Draft tokens: %@" = "초안 토큰: %@";
"Draft window: %@" = "임시 창: %@";
"EPUB viewing not supported on this platform" = "이 플랫폼에서는 EPUB 보기가 지원되지 않습니다.";
"Embedding" = "임베딩";
"Embedding Model Ready" = "임베딩 모델 준비";
"Embedding is resource intensive. For best performance, plug in your phone. Do you want to proceed on battery?" = "포함은 리소스 집약적입니다. 최상의 성능을 얻으려면 휴대폰을 연결하세요. 배터리로 계속하시겠습니까?";
"Enabling Bluetooth…" = "블루투스를 활성화하는 중…";
"Enhance with Datasets" = "데이터 세트로 향상";
"Error" = "오류";
"Estimated working set: %@ · Budget: %@" = "예상 작업 세트: %1$@ · 예산: %2$@";
"Experts Per Token" = "토큰당 전문가";
"Explore Datasets" = "데이터 세트 탐색";
"Expose any downloaded models or connected remote endpoints from the Stored tab to your paired devices. Select which one should answer conversations when the relay is running." = "다운로드한 모델이나 연결된 원격 엔드포인트를 저장됨 탭에서 페어링된 장치에 노출하세요. 릴레이가 실행 중일 때 대화에 응답해야 하는 항목을 선택하세요.";
"Expose to iOS" = "iOS에 노출";
"Explore the latest open-source models optimized for your Mac." = "Mac에 최적화된 최신 오픈 소스 모델을 탐색하세요.";
"Failed to load README" = "README를 로드하지 못했습니다.";
"Failed: %@" = "실패: %@";
"Fastest option on this device: SLM (Leap) models." = "이 장치에서 가장 빠른 옵션: SLM(Leap) 모델.";
"Field requirements will depend on your specific backend deployment." = "필드 요구 사항은 특정 백엔드 배포에 따라 달라집니다.";
"Files" = "파일";
"First, enable fast dataset search" = "먼저, 빠른 데이터세트 검색을 활성화하세요.";
"First-time GGUF load takes longer" = "처음 GGUF 로드 시 시간이 더 오래 걸림";
"First-time download from HuggingFace" = "HuggingFace에서 처음 다운로드";
"First‑time setup: download the Qwen‑1.7B model and embeddings.\nWi‑Fi recommended." = "최초 설정: Qwen‑1.7B 모델 및 임베딩을 다운로드합니다.\nWi-Fi가 권장됩니다.";
"For best performance, please plug in your phone until this completes." = "최상의 성능을 위해서는 완료될 때까지 휴대폰을 연결해 두십시오.";
"Force Local Network" = "강제 로컬 네트워크";
"Forces chat traffic through the last LAN host even if Wi‑Fi names don't match yet." = "Wi-Fi 이름이 아직 일치하지 않는 경우에도 마지막 LAN 호스트를 통해 채팅 트래픽을 강제합니다.";
"Formatted view unavailable" = "형식화된 보기를 사용할 수 없습니다.";
"Found unsupported: %@ …" = "지원되지 않는 것으로 확인됨: %@ …";
"Frequency penalty: %@" = "빈도 패널티: %@";
"GGUF models are the most compatible option. Use the format switch to explore the other builds when you need them." = "GGUF 모델은 가장 호환되는 옵션입니다. 필요할 때 형식 스위치를 사용하여 다른 빌드를 탐색하세요.";
"GGUF works everywhere. MLX targets Apple Silicon speed. SLM focuses on responsiveness on any device." = "GGUF는 어디에서나 작동합니다. MLX는 Apple Silicon 속도를 목표로 합니다. SLM은 모든 장치의 응답성에 중점을 둡니다.";
"GPU Offload Layers" = "GPU 오프로드 계층";
"GPU off-load is not supported for this model." = "이 모델에서는 GPU 오프로드가 지원되지 않습니다.";
"Get Started" = "시작하기";
"Help shape Noema by trying upcoming features and sharing feedback." = "곧 출시될 기능을 사용해 보고 피드백을 공유하여 Noema를 형성하는 데 도움을 주세요.";
"High context lengths use more memory" = "높은 컨텍스트 길이는 더 많은 메모리를 사용합니다.";
"High-quality embedding model for local RAG" = "로컬 RAG를 위한 고품질 임베딩 모델";
"Host ID: %@" = "호스트 ID: %@";
"How it works" = "작동 원리";
"I'm New to Local LLMs, Guide Me" = "저는 지역 LLM이 처음입니다. 안내해 주세요.";
"If enabled, the app will attempt to load models even when they likely exceed your device's memory budget. This can cause the app to terminate." = "활성화되면 앱은 기기의 메모리 예산을 초과할 가능성이 있는 경우에도 모델 로드를 시도합니다. 이로 인해 앱이 종료될 수 있습니다.";
"Import Dataset" = "데이터세트 가져오기";
"Import PDFs, EPUBs, or text files to build local knowledge bases." = "PDF, EPUB 또는 텍스트 파일을 가져와서 로컬 지식 베이스를 구축하세요.";
"Import your own PDFs, EPUBs, or TXT files and keep them local." = "자신의 PDF, EPUB 또는 TXT 파일을 가져와 로컬에 보관하세요.";
"Importing & Scanning..." = "가져오기 및 스캔 중...";
"In progress..." = "진행중...";
"Indexing %@" = "색인 생성 %@";
"Indexing dataset…" = "데이터세트 인덱싱 중…";
"Indexing: %@% · %@" = "색인 생성: %1$@%2$ · %3$@";
"Install a local model to make it available at launch." = "출시 시 사용할 수 있도록 로컬 모델을 설치하세요.";
"Install another model with the same architecture and equal or smaller size to enable speculative decoding." = "추측적 디코딩을 활성화하려면 아키텍처가 동일하고 크기가 같거나 작은 다른 모델을 설치하십시오.";
"K Cache Quant" = "K 캐시 퀀트";
"Keep this iPhone or iPad within a few feet of the Mac that is advertising Noema Relay. We'll pull the relay details automatically once connected." = "Noema Relay를 광고하는 Mac에서 몇 피트 이내에 이 iPhone 또는 iPad를 보관하세요. 연결되면 릴레이 세부 정보를 자동으로 가져옵니다.";
"LAN URL: %@" = "LAN URL: %@";
"Large Model Downloads" = "대형 모델 다운로드";
"Last Sync" = "마지막 동기화";
"Last refreshed %@" = "마지막으로 새로 고친 날짜: %@";
"Latest benchmark" = "최신 벤치마크";
"Latest integrated release: %@" = "최신 통합 릴리스: %@";
"Library" = "도서관";
"Light" = "빛";
"Llama.cpp" = "Call.cpp";
"Load" = "짐";
"Load a local model before chatting. You can download one from the Explore tab or load a model you've already installed." = "채팅하기 전에 로컬 모델을 로드하세요. 탐색 탭에서 다운로드하거나 이미 설치한 모델을 로드할 수 있습니다.";
"Loading recommendation…" = "추천 로드 중…";
"Loading…" = "로드 중…";
"Local Network HTTP server for LAN clients (OpenAI-compatible)" = "LAN 클라이언트용 로컬 네트워크 HTTP 서버(OpenAI 호환)";
"Low = focused. High = varied." = "낮음 = 집중. 높음 = 다양함.";
"Lower = more results (more noise). Higher = stricter matches." = "낮음 = 더 많은 결과(더 많은 노이즈) 높을수록 일치가 더 엄격해집니다.";
"MLX currently manages expert routing automatically; manual selection is not supported." = "MLX는 현재 전문가 라우팅을 자동으로 관리합니다. 수동 선택은 지원되지 않습니다.";
"Many models are several gigabytes in size and require a stable connection and sufficient storage. Downloads can fail or take a long time on slow networks or devices with limited space." = "많은 모델은 크기가 수 기가바이트에 달하며 안정적인 연결과 충분한 저장 공간이 필요합니다. 느린 네트워크나 공간이 제한된 장치에서는 다운로드가 실패하거나 오랜 시간이 걸릴 수 있습니다.";
"Max Chunks: %@" = "최대 청크: %@";
"Max recommended context on this device: ~%@ tokens" = "이 기기의 최대 권장 컨텍스트: ~%@ 토큰";
"Measure real-world generation speed for this configuration. A short scripted prompt will run locally and report timing and memory usage." = "이 구성에 대한 실제 생성 속도를 측정합니다. 짧은 스크립트 프롬프트가 로컬로 실행되고 타이밍과 메모리 사용량을 보고합니다.";
"Min-p" = "최소p";
"Min-p: %@" = "최소 p: %@";
"Minimum cosine similarity a passage must have to be considered relevant. Lower = more passages (higher recall, more noise). Higher = fewer, more precise passages. Try 0.2–0.4 for broad questions; 0.5–0.7 for precise lookups." = "최소 코사인 유사성은 관련성 있는 것으로 간주되어야 합니다. 낮음 = 더 많은 구절(더 높은 재현율, 더 많은 노이즈) 높을수록 = 더 적고 더 정확한 구절. 광범위한 질문에는 0.2~0.4를 사용해 보세요. 정확한 조회를 위해서는 0.5–0.7입니다.";
"MoE layers: %@ / %@" = "MoE 레이어: %1$@ / %2$@";
"Model Detection Limitations" = "모델 감지 제한 사항";
"Model Formats" = "모델 형식";
"Models" = "모델";
"Models shown here are exposed by the Mac relay. Manage sources in the Relay tab on macOS to share more models." = "여기에 표시된 모델은 Mac 릴레이에 의해 노출됩니다. 더 많은 모델을 공유하려면 macOS의 Relay 탭에서 소스를 관리하세요.";
"Move your device closer to the Mac running the relay if it doesn't appear right away. Bluetooth discovery usually completes within a few seconds." = "장치가 즉시 나타나지 않으면 릴레이를 실행하는 Mac에 더 가까이 장치를 이동하십시오. Bluetooth 검색은 일반적으로 몇 초 내에 완료됩니다.";
"Name your dataset" = "데이터세트 이름 지정";
"Nearby Relays" = "인근 릴레이";
"Nearby iPhone and iPad devices discover your Mac relay instantly and sync pairing codes over the air." = "근처의 iPhone 및 iPad 장치는 Mac 릴레이를 즉시 발견하고 무선으로 페어링 코드를 동기화합니다.";
"Need a fresh thread? Tap the plus button for a brand-new conversation." = "새로운 실이 필요하신가요? 새로운 대화를 원하시면 더하기 버튼을 누르세요.";
"Nice! You already have the recommended GGUF starter model ready to use." = "멋진! 이미 권장되는 GGUF 스타터 모델을 사용할 준비가 되어 있습니다.";
"No compatible files found for retrieval. Supported: PDF, EPUB, TXT, MD, JSON, JSONL, CSV, TSV" = "검색할 수 있는 호환 파일이 없습니다. 지원됨: PDF, EPUB, TXT, MD, JSON, JSONL, CSV, TSV";
"No connection responses recorded yet." = "아직 기록된 연결 응답이 없습니다.";
"No datasets available yet. Import or download datasets to build your personal library." = "아직 사용할 수 있는 데이터 세트가 없습니다. 데이터세트를 가져오거나 다운로드하여 개인 라이브러리를 구축하세요.";
"No datasets found. Try different keywords." = "데이터세트를 찾을 수 없습니다. 다른 키워드를 사용해 보세요.";
"No datasets imported yet." = "아직 가져온 데이터세트가 없습니다.";
"No datasets yet" = "아직 데이터세트가 없습니다.";
"No files listed for this dataset." = "이 데이터 세트에 대해 나열된 파일이 없습니다.";
"No model >" = "모델 없음 >";
"No model loaded" = "로드된 모델 없음";
"No models available. Add downloads or remote connections in Stored to configure the relay." = "사용 가능한 모델이 없습니다. 릴레이를 구성하려면 저장됨에 다운로드 또는 원격 연결을 추가하세요.";
"No models cached yet. Open the backend to refresh its catalog." = "아직 캐시된 모델이 없습니다. 백엔드를 열어 카탈로그를 새로 고치세요.";
"No models found for '%@'" = "'%@'에 대한 모델을 찾을 수 없습니다.";
"No models loaded right now. We'll spin one up when a request arrives." = "현재 로드된 모델이 없습니다. 요청이 도착하면 하나를 가동하겠습니다.";
"No models match your search." = "검색어와 일치하는 모델이 없습니다.";
"No models yet" = "아직 모델이 없습니다.";
"No parameters" = "매개변수 없음";
"No quant files available" = "사용 가능한 퀀트 파일이 없습니다.";
"No recent devices. We'll list clients the next time they talk to this relay." = "최근 기기가 없습니다. 다음에 클라이언트가 이 릴레이와 대화할 때 클라이언트 목록을 나열하겠습니다.";
"No remote endpoints configured yet." = "아직 구성된 원격 엔드포인트가 없습니다.";
"No remote endpoints configured." = "구성된 원격 엔드포인트가 없습니다.";
"No ≥Q3 quants are available for this model." = "이 모델에는 ≥Q3 퀀트를 사용할 수 없습니다.";
"Noema" = "십일월";
"Noema REST API — /api/v0/* for model catalog & operations" = "Noema REST API — 모델 카탈로그 및 작업을 위한 /api/v0/*";
"Noema Relay" = "노에마 릴레이";
"Noema Server" = "노에마 서버";
"Noema attempts to gauge available memory to prevent models from exceeding device limits. These checks may occasionally miss risky situations and allow a model to crash your app, or they may be overly conservative and block a model that could have run fine." = "Noema는 모델이 장치 제한을 초과하지 않도록 사용 가능한 메모리를 측정하려고 시도합니다. 이러한 검사는 때때로 위험한 상황을 놓치고 모델이 앱을 충돌시킬 수 있거나 지나치게 보수적이어서 제대로 실행될 수 있는 모델을 차단할 수 있습니다.";
"Noema could not find a projector in the repository. If the model advertises vision, ensure the mmproj file is present in the same folder as the weights." = "Noema가 저장소에서 프로젝터를 찾을 수 없습니다. 모델이 비전을 광고하는 경우 mmproj 파일이 가중치와 동일한 폴더에 있는지 확인하세요.";
"Noema has been reset. The embedding model remains installed." = "노에마가 재설정되었습니다. 임베딩 모델은 설치된 상태로 유지됩니다.";
"Nomic Embed Text v1.5 (Q4_K_M)" = "노믹 삽입 텍스트 v1.5(Q4_K_M)";
"None" = "없음";
"Not found" = "찾을 수 없음";
"Not provided" = "제공되지 않음";
"OK" = "좋아요";
"Off-Grid" = "오프 그리드";
"Off-grid mode blocks every network call so the app stays self-contained. Good luck exploring Noema!" = "오프 그리드 모드는 모든 네트워크 호출을 차단하므로 앱이 독립적으로 유지됩니다. Noema 탐험에 행운을 빕니다!";
"Only one expert is available for this model; the active expert count is fixed." = "이 모델에는 전문가가 한 명만 있습니다. 활성 전문가 수는 고정되어 있습니다.";
"Open Stored to choose a model to run locally or connect to a remote endpoint." = "Stored를 열어 로컬로 실행하거나 원격 엔드포인트에 연결할 모델을 선택하세요.";
"Open the sidebar to revisit any previous session without losing your spot." = "사이드바를 열어 자리를 잃지 않고 이전 세션을 다시 방문하세요.";
"OpenAI-style API — /v1/chat/completions, /v1/completions, /v1/models" = "OpenAI 스타일 API — /v1/chat/completions, /v1/completions, /v1/models";
"Optimizations in use" = "사용 중인 최적화";
"PDF viewing not supported on this platform" = "이 플랫폼에서는 PDF 보기가 지원되지 않습니다.";
"Pick a model and add a dataset" = "모델 선택 및 데이터 세트 추가";
"Pick the SLM format when you want ultra-responsive models that run well anywhere." = "어디서나 잘 실행되는 응답성이 뛰어난 모델을 원할 경우 SLM 형식을 선택하십시오.";
"Pinned answer" = "고정된 답변";
"Pinned answer unavailable" = "고정된 답변을 사용할 수 없습니다.";
"Preparation" = "준비";
"Preparing Embedding Model" = "임베딩 모델 준비";
"Preparing benchmark…" = "벤치마크 준비 중…";
"Preparing…" = "준비 중…";
"Presence penalty: %@" = "출석 패널티: %@";
"Projector (mmproj)" = "프로젝터(mmproj)";
"Projector downloaded automatically from Hugging Face. Keep this file alongside the weights so vision remains available." = "Hugging Face에서 프로젝터가 자동으로 다운로드되었습니다. 시력을 계속 사용할 수 있도록 이 파일을 가중치와 함께 보관하세요.";
"Quantize the runtime key cache to save memory. Experimental." = "런타임 키 캐시를 양자화하여 메모리를 절약합니다. 실험적입니다.";
"Quantize the runtime value cache to save memory when Flash Attention is enabled. Experimental." = "Flash Attention이 활성화된 경우 메모리를 절약하기 위해 런타임 값 캐시를 양자화합니다. 실험적입니다.";
"Qwen 3 1.7B GGUF (Q3_K_M) gives you a dependable starting point. Delete it anytime if you need space." = "Qwen 3 1.7B GGUF(Q3_K_M)는 신뢰할 수 있는 시작점을 제공합니다. 공간이 필요하면 언제든지 삭제하세요.";
"RAG embeds normalized paragraphs from your PDFs and EPUBs. On each question, the most relevant chunks are retrieved and added to the prompt. Images are ignored." = "RAG는 ​​PDF 및 EPUB의 정규화된 단락을 포함합니다. 각 질문에서 가장 관련성이 높은 청크가 검색되어 프롬프트에 추가됩니다. 이미지는 무시됩니다.";
"RAM Safety Checks" = "RAM 안전 점검";
"RAM information for this device will be added in a future update." = "이 장치의 RAM 정보는 향후 업데이트에 추가될 예정입니다.";
"REST Endpoints" = "REST 엔드포인트";
"Reachable at" = "접속 가능 장소";
"Ready for Use" = "사용 준비 완료";
"Recent Chats" = "최근 채팅";
"Recommended" = "추천";
"Recommended Starter Model" = "권장 스타터 모델";
"Relay ID" = "릴레이 ID";
"Relay Server Running" = "릴레이 서버 실행 중";
"Relay Sources" = "릴레이 소스";
"Remaining: %@" = "남은 수: %@";
"Remember:" = "기억하다:";
"Remote Backends" = "원격 백엔드";
"Remote endpoint is offline. This model can't be found at this time." = "원격 끝점이 오프라인입니다. 현재 이 모델을 찾을 수 없습니다.";
"Remote timeout: %@s" = "원격 시간 초과: %@s";
"Repeat last N tokens: %@" = "마지막 N개 토큰 반복: %@";
"Repetition penalty: %@" = "반복 페널티: %@";
"Request Parameters" = "요청 매개변수";
"Responses are generated by the macOS relay server. Configure the provider (LM Studio or Ollama) on the Mac app." = "응답은 macOS 릴레이 서버에서 생성됩니다. Mac 앱에서 공급자(LM Studio 또는 Ollama)를 구성합니다.";
"Result" = "결과";
"Results" = "결과";
"Save" = "구하다";
"Saving…" = "절약…";
"Score: %@" = "점수: %@";
"SearXNG web search is available without limits. There's nothing to purchase—just enable the globe button in chat whenever you need online results." = "SearXNG 웹 검색은 제한 없이 이용 가능합니다. 구매할 것이 없습니다. 온라인 결과가 필요할 때마다 채팅에서 지구본 버튼을 활성화하기만 하면 됩니다.";
"SearXNG web search is enabled for this device." = "이 장치에서는 SearXNG 웹 검색이 활성화되어 있습니다.";
"Search" = "찾다";
"Search for any subject you're interested in." = "관심 있는 주제를 검색해 보세요.";
"Search requests are proxied through https://search.noemaai.com and are available without quotas." = "검색 요청은 https://search.noemaai.com을 통해 프록시되며 할당량 없이 사용할 수 있습니다.";
"Seed" = "씨앗";
"Selecting more experts keeps additional expert weights resident in RAM and increases memory usage." = "더 많은 전문가를 선택하면 RAM에 추가 전문가 가중치가 유지되고 메모리 사용량이 늘어납니다.";
"Server Settings" = "서버 설정";
"Share Logs" = "로그 공유";
"Sharing relay payload with nearby devices…" = "근처 기기와 릴레이 페이로드를 공유하는 중…";
"Shows the last server response." = "마지막 서버 응답을 표시합니다.";
"Signal" = "신호";
"Similarity Threshold" = "유사성 임계값";
"Simple" = "단순한";
"Smooth loops and phrase echo by balancing repetition controls." = "반복 컨트롤의 균형을 맞춰 루프와 프레이즈 에코를 부드럽게 합니다.";
"Smooth loops and repeated phrases by tuning repetition controls." = "반복 컨트롤을 조정하여 루프와 반복 문구를 부드럽게 만드세요.";
"Some models do not provide the system prompts needed for Noema to detect and configure them properly. These models may be unusable until they include appropriate metadata or support." = "일부 모델은 Noema가 이를 적절하게 감지하고 구성하는 데 필요한 시스템 프롬프트를 제공하지 않습니다. 이러한 모델은 적절한 메타데이터나 지원이 포함될 때까지 사용하지 못할 수 있습니다.";
"Source unavailable. Check storage or network settings." = "소스를 사용할 수 없습니다. 저장소나 네트워크 설정을 확인하세요.";
"Source: %@" = "출처: %@";
"Specify identifiers for models that are not listed by the server. Leave blank to rely on the server's catalog." = "서버에 나열되지 않은 모델의 식별자를 지정합니다. 서버 카탈로그를 사용하려면 비워 두세요.";
"Specify your model identifiers, or reload your custom models later." = "모델 식별자를 지정하거나 나중에 사용자 정의 모델을 다시 로드하세요.";
"Speed up with a smaller helper model." = "더 작은 도우미 모델로 속도를 높이세요.";
"Start by installing one model. Then add a dataset (like an open textbook) so the AI can answer with grounded knowledge." = "하나의 모델을 설치하여 시작하십시오. 그런 다음 AI가 근거 있는 지식으로 답변할 수 있도록 데이터 세트(예: 공개 교과서)를 추가합니다.";
"Start the relay to automatically share the latest payload with nearby devices." = "근처 장치와 최신 페이로드를 자동으로 공유하려면 릴레이를 시작하세요.";
"Start with a reliable Qwen 3 1.7B build. It balances capability with small download size." = "안정적인 Qwen 3 1.7B 빌드로 시작하세요. 작은 다운로드 크기와 기능의 균형을 유지합니다.";
"Startup defaults now live in Settings → Startup. Favorite models here to keep them handy." = "이제 시작 기본값이 설정 → 시작에 적용됩니다. 여기에 즐겨찾는 모델을 보관해 두세요.";
"Status" = "상태";
"Stay close to your Mac" = "Mac을 가까이에 두세요";
"Stop Using Dataset" = "데이터 세트 사용 중지";
"Streaming" = "스트리밍";
"Streaming response…" = "스트리밍 응답…";
"Supported Endpoints" = "지원되는 엔드포인트";
"Supported formats: PDF, EPUB, TXT, MD, JSON, JSONL, CSV, TSV" = "지원되는 형식: PDF, EPUB, TXT, MD, JSON, JSONL, CSV, TSV";
"Swap between the new stacked chat panel and the classic tab bar layout." = "새로운 스택형 채팅 패널과 클래식 탭 표시줄 레이아웃 간을 전환합니다.";
"Swipe left to remove the embedding model from this device." = "이 기기에서 임베딩 모델을 삭제하려면 왼쪽으로 스와이프하세요.";
"Switch the selector to MLX for Apple Silicon‑optimized builds that excel at speed." = "속도가 뛰어난 Apple Silicon 최적화 빌드를 위해 선택기를 MLX로 전환하세요.";
"Switch to Raw to inspect the original response." = "원본 응답을 검사하려면 Raw로 전환하세요.";
"System" = "체계";
"Tap to load" = "탭하여 로드";
"Temperature" = "온도";
"Testing" = "테스트";
"The app memory usage budget is an estimate based on your device's total RAM and typical iOS memory management. The actual available memory may vary depending on system load, other running apps, and iOS memory pressure. Models that exceed this budget may cause the app to be terminated by iOS." = "앱 메모리 사용량 예산은 기기의 총 RAM 및 일반적인 iOS 메모리 관리를 기반으로 한 추정치입니다. 실제 사용 가능한 메모리는 시스템 로드, 실행 중인 다른 앱, iOS 메모리 압력에 따라 달라질 수 있습니다. 이 예산을 초과하는 모델은 iOS에 의해 앱이 종료될 수 있습니다.";
"The embedding model is installed. Delete it to free ~320 MB." = "임베딩 모델이 설치되었습니다. ~320MB의 여유 공간을 확보하려면 삭제하세요.";
"The relay listens to the %@ container for new conversations and responds with your selected provider." = "릴레이는 새로운 대화를 위해 %@ 컨테이너를 수신하고 선택한 공급자에게 응답합니다.";
"The tool returned data that can't be formatted. Switch to Raw to inspect the original response." = "도구에서 형식을 지정할 수 없는 데이터를 반환했습니다. 원본 응답을 검사하려면 Raw로 전환하세요.";
"These options stay in simple mode for clarity. Let’s cover the essentials." = "이러한 옵션은 명확성을 위해 단순 모드로 유지됩니다. 필수 사항을 다루겠습니다.";
"Think of Noema as a simple way to run AI on your device. To get useful answers, you pair a local model with datasets (like open textbooks). We’ll guide you through the first setup." = "Noema를 장치에서 AI를 실행하는 간단한 방법으로 생각하십시오. 유용한 답변을 얻으려면 로컬 모델을 데이터 세트(예: 공개 교과서)와 결합하세요. 첫 번째 설정을 안내해 드리겠습니다.";
"This app bundles llama.cpp; we keep this in sync with upstream b‑releases." = "이 앱은 llama.cpp를 번들로 제공합니다. 우리는 이것을 업스트림 b-릴리스와 동기화 상태로 유지합니다.";
"This backend is unavailable. Remove it or pick another option." = "이 백엔드를 사용할 수 없습니다. 제거하거나 다른 옵션을 선택하세요.";
"This chat stays private—responses are generated on your device after you load a model." = "이 채팅은 비공개로 유지됩니다. 모델을 로드한 후 장치에서 응답이 생성됩니다.";
"This configuration exceeds the current RAM safety guard, so benchmarking is disabled." = "이 구성은 현재 RAM 안전 가드를 초과하므로 벤치마킹이 비활성화됩니다.";
"This dataset is taking a while to load, still working…" = "이 데이터 세트를 로드하는 데 시간이 걸리고 있지만 여전히 작동 중입니다…";
"This dataset's files are not currently supported for document retrieval." = "이 데이터 세트의 파일은 현재 문서 검색이 지원되지 않습니다.";
"This device doesn't support GPU offload." = "이 장치는 GPU 오프로드를 지원하지 않습니다.";
"This device doesn't support GPU offload; GGUF models will run on the CPU and generation speed will be significantly slower." = "이 장치는 GPU 오프로드를 지원하지 않습니다. GGUF 모델은 CPU에서 실행되며 생성 속도가 상당히 느려집니다.";
"This model doesn't support GPU offload and generation speed will be significantly slower. Consider switching to an MLX model." = "이 모델은 GPU 오프로드를 지원하지 않으며 생성 속도가 상당히 느려집니다. MLX 모델로 전환해 보세요.";
"This model doesn't support GPU offload and generation speed will be significantly slower. Fastest option on this device: use an SLM (Leap) model." = "이 모델은 GPU 오프로드를 지원하지 않으며 생성 속도가 상당히 느려집니다. 이 장치에서 가장 빠른 옵션: SLM(Leap) 모델을 사용합니다.";
"This model doesn't support GPU offload and may run slowly. Consider an MLX model." = "이 모델은 GPU 오프로드를 지원하지 않으며 느리게 실행될 수 있습니다. MLX 모델을 고려해보세요.";
"This model doesn't support GPU offload and may run slowly. Fastest option: use an SLM model." = "이 모델은 GPU 오프로드를 지원하지 않으며 느리게 실행될 수 있습니다. 가장 빠른 옵션: SLM 모델을 사용합니다.";
"This permanently removes every chat conversation. This action cannot be undone." = "이렇게 하면 모든 채팅 대화가 영구적으로 제거됩니다. 이 작업은 취소할 수 없습니다.";
"This textbook appears to be available only as a web page. Noema can't import it as a dataset." = "이 교과서는 웹페이지로만 제공되는 것으로 보입니다. Noema에서는 이를 데이터세트로 가져올 수 없습니다.";
"Tool calling isn't perfect. Although Noema implements many methods of detecting and instructing models to use tools, not all LLMs will follow instructions and some might not call them correctly or at all. Tool calling heavily depends on model pre-training and will get better as time passes." = "도구 호출이 완벽하지 않습니다. Noema는 모델이 도구를 사용하도록 감지하고 지시하는 다양한 방법을 구현하지만 모든 LLM이 지침을 따르는 것은 아니며 일부는 올바르게 호출하지 않거나 전혀 호출하지 않을 수도 있습니다. 도구 호출은 모델 사전 훈련에 크게 의존하며 시간이 지나면서 더 좋아질 것입니다.";
"Tools" = "도구";
"Top-k: %@" = "탑케이: %@";
"Top-p" = "탑피";
"Top-p: %@" = "상단: %@";
"Try again" = "다시 시도하세요";
"Try another dataset if these formats aren't available." = "이러한 형식을 사용할 수 없는 경우 다른 데이터세트를 사용해 보세요.";
"Try the Qwen 3 1.7B GGUF (Q3_K_M) build below. It's a good starting point and you can delete it anytime." = "아래의 Qwen 3 1.7B GGUF(Q3_K_M) 빌드를 사용해 보세요. 이는 좋은 시작점이며 언제든지 삭제할 수 있습니다.";
"Unable to load image" = "이미지를 로드할 수 없습니다.";
"Unknown error" = "알 수 없는 오류";
"Updates every second" = "매초 업데이트";
"Use" = "사용";
"Use Dataset" = "데이터세트 사용";
"Use this switch to flip between finding models or datasets." = "이 스위치를 사용하여 모델 또는 데이터세트 찾기 간에 전환합니다.";
"Using %@" = "%@ 사용";
"Using %@ of %@ budget" = "%2$@ 예산 중 %1$@ 사용";
"Using more than %@ significantly increases RAM usage." = "%@ 이상을 사용하면 RAM 사용량이 크게 늘어납니다.";
"Using server catalog" = "서버 카탈로그 사용";
"V Cache Quant" = "V 캐시 퀀트";
"Vendor recommendation: %@" = "공급업체 추천: %@";
"Version" = "버전";
"Version %@" = "버전 %@";
"Version 1.4" = "버전 1.4";
"Vision models require a companion projector (.mmproj). Noema will fetch it automatically the next time you download this model." = "Vision 모델에는 동반 프로젝터(.mmproj)가 필요합니다. Noema는 다음에 이 모델을 다운로드할 때 자동으로 가져옵니다.";
"Wait for the response in your other chat to finish before sending a new message." = "새 메시지를 보내기 전에 다른 채팅의 응답이 완료될 때까지 기다리세요.";
"Waiting for tool response…" = "도구 응답을 기다리는 중…";
"We'll extract text and prepare embeddings. You can also start later from the dataset details." = "텍스트를 추출하고 임베딩을 준비하겠습니다. 나중에 데이터세트 세부정보에서 시작할 수도 있습니다.";
"We'll route new conversations through %@ even if Wi‑Fi names differ. You can switch back by reloading the backend." = "Wi‑Fi 이름이 다르더라도 %@을(를) 통해 새 대화를 라우팅합니다. 백엔드를 다시 로드하여 다시 전환할 수 있습니다.";
"We'll try remote models in priority order for this long before moving to the next option." = "다음 옵션으로 넘어가기 전에 오랫동안 우선순위에 따라 원격 모델을 시도해 보겠습니다.";
"We'll try this saved identifier even though it's not in the latest catalog." = "최신 카탈로그에 없더라도 저장된 식별자를 사용해 보겠습니다.";
"Web Search Tool Calls" = "웹 검색 도구 호출";
"Web Search button" = "웹 검색 버튼";
"Web search is included" = "웹 검색이 포함되어 있습니다";
"Weights" = "가중치";
"Welcome to Noema" = "노에마에 오신 것을 환영합니다";
"Welcome to Noema for Mac" = "Mac용 Noema에 오신 것을 환영합니다.";
"What is Noema?" = "노에마란 무엇인가요?";
"When connecting from another device, point the base URL to your computer (for example http://192.168.0.10:11434) and start Ollama with `OLLAMA_HOST=0.0.0.0` so it accepts remote clients." = "다른 장치에서 연결할 때 기본 URL을 컴퓨터(예: http://192.168.0.10:11434)로 지정하고 `OLLAMA_HOST=0.0.0.0`으로 Ollama를 시작하여 원격 클라이언트를 허용합니다.";
"When enabled, pressing eject on iOS tells this Mac to unload the active relay model." = "활성화된 경우 iOS에서 꺼내기를 누르면 이 Mac에 활성 릴레이 모델이 언로드됩니다.";
"When enabled, you will be asked to choose parameters every time a model loads." = "활성화하면 모델이 로드될 때마다 매개변수를 선택하라는 메시지가 표시됩니다.";
"Wi-Fi: %@" = "WiFi: %@";
"Working set estimate (%@): %@ @ %@ tokens" = "작업 세트 추정(%1$@): %2$@ @ %3$@ 토큰";
"Write prompts, instructions, or notes here. Press return to add new lines." = "여기에 프롬프트, 지침 또는 메모를 작성하세요. 새 줄을 추가하려면 Return 키를 누르세요.";
"You can keep chatting while indexing finishes" = "색인 생성이 완료되는 동안 계속 채팅할 수 있습니다.";
"You can only favorite up to three models." = "최대 3개 모델까지만 즐겨찾기에 등록할 수 있습니다.";
"You can restart this process in the dataset settings any time." = "언제든지 데이터 세트 설정에서 이 프로세스를 다시 시작할 수 있습니다.";
"You're in Off-Grid mode. The Explore tab is hidden and all network features are disabled. You can only use downloaded models and datasets." = "오프 그리드 모드에 있습니다. 탐색 탭이 숨겨지고 모든 네트워크 기능이 비활성화됩니다. 다운로드한 모델과 데이터 세트만 사용할 수 있습니다.";
"Your Datasets" = "귀하의 데이터 세트";
"Your Models" = "귀하의 모델";
"Your private AI workspace" = "개인 AI 작업 공간";
"You’re ready to explore. Download models, add datasets, and start chatting." = "탐험할 준비가 되었습니다. 모델을 다운로드하고, 데이터 세트를 추가하고, 채팅을 시작하세요.";
"minutes" = "분";
"•" = "";
"• Close other applications to free up RAM" = "• 다른 애플리케이션을 닫아 RAM 여유 공간 확보";
"• Embedding happens locally on your device" = "• 삽입은 기기에서 로컬로 이루어집니다.";
"• Larger datasets take exponentially more time" = "• 데이터 세트가 클수록 시간이 기하급수적으로 더 많이 소요됩니다.";
"• You can pause and resume downloads if needed" = "• 필요한 경우 다운로드를 일시중지하고 재개할 수 있습니다.";
"…and %@ more parameter%@" = "...그리고 %1$@ 더 많은 매개변수%2$@";
"⚠️ " = "";
"General" = "일반";

"Privacy" = "개인정보";

"About" = "정보";

"About & Support" = "정보 및 지원";

"Network" = "네트워크";

"Embedding Model" = "임베딩 모델";

"Retrieval" = "리트리벌";

"Early Testers" = "초기 테스터";

"Build Info" = "빌드 정보";

"Settings" = "설정";

"Language" = "언어";
"Startup" = "시작";
"Search models" = "모델 검색";
"SLM Models - Liquid AI" = "SLM Models - Liquid AI";
"Import" = "가져오기";
"Import GGUF" = "GGUF 가져오기";
"Import MLX" = "MLX 가져오기";
"Import Failed" = "가져오기 실패";
"Switching between GGUF/MLX modes" = "GGUF/MLX 모드 전환";
"Switching between GGUF/SLM modes" = "GGUF/SLM 모드 전환";
"Vision-capable model" = "비전 지원 모델";
"All" = "전체";
"Text" = "텍스트";
"Vision" = "비전";
"Continue" = "계속";
"Try bullet" = "다음을 시도하세요:\n• 다른 키워드(예: 'gemma 3' 대신 'gemma-3')\n• %@\n• 텍스트/비전 필터 조정\n• 검색 필터 확인";
"Select one or more .gguf files. For vision models, also select the projector .gguf (files containing ‘mmproj’ or ‘projector’). Files will be copied into your local model library." = "Select one or more .gguf files. For vision models, also select the projector .gguf (files containing ‘mmproj’ or ‘projector’). Files will be copied into your local model library.";
"Select the model folder that contains config.json and the weights (safetensors/npz). The entire folder will be imported into your local model library." = "Select the model folder that contains config.json and the weights (safetensors/npz). The entire folder will be imported into your local model library.";
"K Cache Quantization" = "K 캐시 양자화";
"V Cache Quantization" = "V 캐시 양자화";
"Favorite Limit Reached" = "즐겨찾기 한도에 도달";
"Overview" = "개요";
"Sampling" = "샘플링";
"Speculative Decoding" = "추측 디코딩";
"Benchmark" = "벤치마크";
"Maintenance" = "유지 관리";
"MLX" = "MLX";
"Tokenizer Path (tokenizer.json)" = "토크나이저 경로 (tokenizer.json)";
"Back" = "뒤로";
"Favorite Model" = "즐겨찾는 모델";
"Reset to Default Settings" = "기본 설정으로 재설정";
"Delete Model" = "모델 삭제";
"Delete %@?" = "%@을(를) 삭제하시겠습니까?";
"Not provided by repository" = "저장소에서 제공되지 않음";
"Unknown (not checked yet)" = "알 수 없음(아직 확인 안 됨)";
"Keep Model In Memory" = "모델을 메모리에 유지";
"GPU Offload Layers: %@/%@" = "GPU 오프로드 레이어: %1$@/%2$@";
"CPU Threads: %@" = "CPU 스레드: %@";
"Offload KV Cache to GPU" = "KV 캐시를 GPU로 오프로드";
"Use mmap()" = "mmap() 사용";
"Random" = "무작위";
"Flash Attention" = "플래시 어텐션";
"1 expert" = "전문가 1명";
"%@ experts" = "%@명 전문가";
"Helper Model" = "보조 모델";
"Draft strategy" = "드래프트 전략";
"Run Benchmark" = "벤치마크 실행";
"Benchmarking…" = "벤치마크 중…";
"Leap SLM models manage runtime optimizations automatically." = "Leap SLM 모델은 런타임 최적화를 자동으로 관리합니다.";
"This format doesn't expose tunable runtime optimizations." = "이 형식은 조정 가능한 런타임 최적화를 제공하지 않습니다.";
"Token processing" = "토큰 처리";
"Token generation" = "토큰 생성";
"Total time" = "총 시간";
"First token" = "첫 토큰";
"Peak memory" = "최대 메모리";
"Output tokens" = "출력 토큰";
"End Guide" = "가이드 종료";
"Streaming benchmark output…" = "벤치마크 출력 스트리밍 중…";
"Streaming… %@ chunks (~%@ tok est.)" = "스트리밍 중… %1$@ 청크(~%2$@ 토큰 추정)";
"Streaming… %d chunks (~%d tok est.)" = "스트리밍 중… %d 청크(~%d 토큰 추정)";
"The selected model's weights could not be located." = "선택한 모델의 가중치를 찾을 수 없습니다.";
"Failed to load model for benchmark: %@" = "벤치마크용 모델 로드 실패: %@";
"Benchmark generation failed: %@" = "벤치마크 생성 실패: %@";
"K Cache" = "K 캐시";
"V Cache" = "V 캐시";
"KV Offload" = "KV 오프로드";
"On" = "켜짐";
"Off" = "꺼짐";
"GPU" = "GPU";
"CPU" = "CPU";
"%.1f tok/s" = "%.1f tok/s";
"%.1fs" = "%.1fs";
"%.2fs" = "%.2fs";
"Move Up" = "위로 이동";
"Move Down" = "아래로 이동";
"Remove" = "제거";
"Startup remote options" = "시작 시 원격 옵션";
"No models cached yet. Open the backend to refresh its catalog." = "아직 캐시된 모델이 없습니다. 백엔드를 열어 카탈로그를 새로 고치세요.";
"We'll try this saved identifier even though it's not in the latest catalog." = "최신 카탈로그에 없더라도 저장된 이 식별자를 사용해 보겠습니다.";
"This backend is unavailable. Remove it or pick another option." = "이 백엔드를 사용할 수 없습니다. 제거하거나 다른 옵션을 선택하세요.";
"Backend removed" = "백엔드가 제거되었습니다";
"What is Max Chunks?" = "최대 청크란?";
"What is Similarity Threshold?" = "유사도 임계값이란?";
"Approx. %@" = "약 %@";
"Advanced mode shows developer options and diagnostics." = "고급 모드는 개발자 옵션과 진단을 표시합니다.";
"Simple mode hides advanced settings for a cleaner interface." = "단순 모드는 인터페이스를 깔끔하게 하기 위해 고급 설정을 숨깁니다.";
"Hide advanced controls" = "고급 컨트롤 숨기기";
"Show advanced controls" = "고급 컨트롤 표시";
"Adjust model settings" = "모델 설정 조정";
"Model Settings" = "모델 설정";
"SLM models are not supported on this platform." = "이 플랫폼에서는 SLM 모델을 지원하지 않습니다.";
"Model likely exceeds memory budget. Lower context or choose a smaller quant." = "모델이 메모리 예산을 초과할 수 있습니다. 컨텍스트를 줄이거나 더 작은 양자화를 선택하세요.";
"Apple bundle models aren't supported on macOS yet." = "Apple 번들 모델은 아직 macOS에서 지원되지 않습니다.";
"Estimate: %@\nBudget: %@\nContext length: %@ tokens\n\nThis is an estimate based on your device’s memory budget, context length (KV cache), and typical runtime overheads. Actual usage may vary." = "추정: %@\n예산: %@\n컨텍스트 길이: %@ 토큰\n\n이는 기기의 메모리 예산, 컨텍스트 길이(KV 캐시) 및 일반적인 런타임 오버헤드를 기반으로 한 추정치입니다. 실제 사용량은 달라질 수 있습니다.";
"Model likely fits in RAM" = "모델이 RAM에 들어갈 것으로 보입니다";
"Model may not fit in RAM" = "모델이 RAM에 들어가지 않을 수 있습니다";
"Fits in RAM (estimated)" = "RAM에 적합(추정)";
"May not fit (estimated)" = "적합하지 않을 수 있음(추정)";
"Please provide a backend name." = "백엔드 이름을 입력하세요.";
"A backend with this name already exists." = "이 이름의 백엔드가 이미 있습니다.";
"Backend not found." = "백엔드를 찾을 수 없습니다.";
"This Noema Relay device is already configured." = "이 Noema Relay 기기는 이미 설정되었습니다.";
"OpenAI API" = "OpenAI API";
"LM Studio" = "LM Studio";
"Ollama" = "Ollama";
"Cloud Relay" = "Cloud Relay";
"Noema Relay" = "Noema Relay";
"Compatible with OpenAI-style /v1 endpoints" = "OpenAI 스타일 /v1 엔드포인트와 호환됩니다";
"Connect to LM Studio's REST server" = "LM Studio의 REST 서버에 연결";
"Target an Ollama host for chat and pulls" = "채팅과 가져오기를 위해 Ollama 호스트를 지정";
"Use Noema's Cloud Relay on macOS" = "macOS에서 Noema의 Cloud Relay 사용";
"Pair with your Mac over CloudKit" = "CloudKit을 통해 Mac과 페어링";
"Please provide the CloudKit container identifier." = "CloudKit 컨테이너 ID를 입력하세요.";
"Please provide the host device ID from the Mac relay." = "Mac 릴레이의 호스트 장치 ID를 입력하세요.";
"Missing host device ID for Noema Relay." = "Noema Relay의 호스트 장치 ID가 없습니다.";
"Missing CloudKit container identifier." = "CloudKit 컨테이너 ID가 없습니다.";
"Relay catalog unavailable." = "릴레이 카탈로그를 사용할 수 없습니다.";
"Relay catalog is still syncing. Open the Mac relay, ensure it is signed into iCloud, then try again in a moment." = "릴레이 카탈로그를 동기화 중입니다. Mac에서 릴레이를 열고 iCloud에 로그인되어 있는지 확인한 후 다시 시도하세요.";
"Terms of Use" = "이용 약관";
"Privacy Policy" = "개인정보 처리방침";
"Contact Support" = "지원 문의";
"Write a Review" = "리뷰 작성";
"Notes & Issues" = "메모 및 문제";
"Qwen3-1.7B is a compact and efficient model from the Qwen3 family, suitable for on-device usage with strong general capabilities." = "Qwen3-1.7B는 Qwen3 패밀리의 작고 효율적인 모델로, 강력한 일반 능력과 함께 온디바이스 사용에 적합합니다.";
"Gemma 3n E2B is a lightweight instruction-tuned model from Google's Gemma family, optimized for efficient on-device conversations." = "Gemma 3n E2B는 Google Gemma 패밀리의 경량 지시 조정 모델로, 효율적인 로컬 대화를 위해 최적화되었습니다.";
"Gemma 3n E2B is an instruction-tuned variant of Google's Gemma family built for efficient reasoning on low-resource devices.\nAvailable in GGUF quants (Q3_K_M, Q4_K_M, Q6_K) and an MLX 4-bit build for Apple Silicon.\n" = "Gemma 3n E2B는 리소스가 제한된 장치에서 효율적인 추론을 위해 설계된 Gemma 패밀리의 지시 특화 버전입니다.\nGGUF 양자화(Q3_K_M, Q4_K_M, Q6_K)와 Apple Silicon용 MLX 4비트 빌드가 제공됩니다.\n";
"Phi-4 Mini Reasoning is a lightweight model from the Phi-4 family, tuned for strong reasoning and efficiency across tasks." = "Phi-4 Mini Reasoning은 Phi-4 패밀리의 경량 모델로, 뛰어난 추론과 효율성에 맞춰 조정되었습니다.";
"Phi-4 Mini Reasoning — a compact model in Microsoft’s Phi-4 line designed for logical reasoning, problem solving, and instruction-following. \nDistributed in efficient GGUF quants (Q3_K_L, Q4_K_M, Q6_K) and an MLX 4-bit variant for Apple Silicon devices.\n" = "Phi-4 Mini Reasoning — Microsoft Phi-4 라인의 컴팩트한 모델로, 논리적 추론, 문제 해결 및 지시 수행을 위해 설계되었습니다.\n효율적인 GGUF 양자화(Q3_K_L, Q4_K_M, Q6_K)와 Apple Silicon용 MLX 4비트 버전으로 제공됩니다.\n";
"Runtime Safety" = "런타임 안전";
"Bypass RAM safety check (may cause crashes)" = "RAM 안전 검사 건너뛰기(충돌 가능)";
"Estimate for" = "다음에 대한 추정";
"Off-grid Mode" = "오프그리드 모드";
"Delete All Chats" = "모든 채팅 삭제";
"Reset App Data" = "앱 데이터 재설정";
"Max Chunks" = "최대 청크";
"Delete Embedding Model" = "임베딩 모델 삭제";
"Override the app language. Defaults to the device language on first launch." = "앱 언어를 재정의합니다. 첫 실행 시 기본적으로 기기 언어가 사용됩니다.";
"Swap between the new stacked chat panel and the classic tab bar layout." = "새 누적 채팅 패널과 기존 탭 바 레이아웃을 전환합니다.";
"Available Quantizations" = "사용 가능한 양자화";
"Sort quantizations" = "양자화 정렬";
"Quant" = "양자";
"Size ↑" = "크기 ↑";
"Size ↓" = "크기 ↓";
"Model Library" = "모델 라이브러리";
"Type to filter models…" = "모델을 필터링하려면 입력…";
"No models match your search." = "검색과 일치하는 모델이 없습니다.";
"Browse Explore tab" = "Explore 탭 열기";
"Manually choose parameters" = "매개변수를 수동으로 선택";
"The base URL looks invalid. Please include the host (e.g. http://127.0.0.1:1234)." = "기본 URL이 올바르지 않습니다. 호스트를 포함하세요(예: http://127.0.0.1:1234).";
"Could not build the remote endpoint URL." = "원격 엔드포인트 URL을 생성할 수 없습니다.";
"The server returned an unexpected response." = "서버가 예기치 않은 응답을 반환했습니다.";
"Server responded with status code %d." = "서버 응답 상태 코드 %d.";
"Server responded with status code %d: %@" = "서버 상태 코드 %1$d: %2$@";
"Failed to decode server response." = "서버 응답을 디코딩하지 못했습니다.";

"Explore" = "탐색";
"Search datasets" = "데이터세트 검색";
"Download" = "다운로드";
"Offline" = "오프라인";
"Tap to load" = "탭하여 불러오기";
"%d models" = "모델 %d개";
"Updated %@" = "%@에 업데이트";
"No models fetched yet" = "아직 모델을 가져오지 않았습니다";
"Auth" = "인증";
"Local Network" = "로컬 네트워크";
"Direct" = "직접 연결";
"LAN" = "LAN";
"LAN · %@" = "LAN · %@";
"Backend" = "백엔드";
"Base URL" = "기본 URL";
"Chat Path" = "채팅 경로";
"Models Path" = "모델 경로";
"Endpoint Type" = "엔드포인트 유형";
"Endpoints" = "엔드포인트";
"Authentication" = "인증";
"Model Identifiers" = "모델 식별자";
"Name" = "이름";
"Host device ID" = "호스트 기기 ID";
"Host Device ID" = "호스트 기기 ID";
"CloudKit container identifier" = "CloudKit 컨테이너 ID";
"Field requirements will depend on your specific backend deployment." = "필드 요구 사항은 백엔드 배포에 따라 달라집니다.";
"Uses Noema Relay configuration" = "Noema Relay 구성 사용";
"Chat: %@\nModels: %@" = "채팅: %@\n모델: %@";

"Downloaded" = "다운로드됨";
"Compressed Text" = "압축된 텍스트";
"Small" = "작음";
"Medium" = "중간";
"Large" = "큼";
"Very Large" = "매우 큼";
"Extreme" = "극대";
"Under 10 MB" = "10MB 미만";
"10–50 MB" = "10~50MB";
"50–200 MB" = "50~200MB";
"200–500 MB" = "200~500MB";
"Over 500 MB" = "500MB 초과";
"Estimated Embedding Time" = "예상 임베딩 시간";
"Peak RAM Usage" = "최대 RAM 사용량";
"Dataset Size" = "데이터셋 크기";
"Performance Note" = "성능 메모";
"Recommendation" = "추천";
"Remember:" = "기억하세요:";
"• Close other applications to free up RAM" = "• RAM을 확보하려면 다른 앱을 종료하세요";
"• Embedding happens locally on your device" = "• 임베딩은 기기에서 로컬로 수행됩니다";
"• Larger datasets take exponentially more time" = "• 더 큰 데이터셋은 시간이 기하급수적으로 증가합니다";
"• You can pause and resume downloads if needed" = "• 필요 시 다운로드를 일시중지하고 다시 시작할 수 있습니다";
"Dataset Requirements" = "데이터셋 요구사항";
"Got it" = "알겠어요";
"Check Requirements" = "요구사항 확인";
"< 1 minute" = "1분 미만";
"%d minutes" = "%d분";
"This dataset should embed quickly with minimal resource usage. Perfect for testing and quick experiments." = "이 데이터셋은 최소한의 자원으로 빠르게 임베딩될 것입니다. 테스트와 빠른 실험에 적합합니다.";
"This dataset is a reasonable size for most systems. Embedding should complete in a few minutes." = "이 데이터셋은 대부분의 시스템에 적당한 크기입니다. 몇 분 안에 임베딩이 완료될 것입니다.";
"This is a substantial dataset. Ensure you have adequate RAM and expect embedding to take 10–30 minutes." = "상당히 큰 데이터셋입니다. 충분한 RAM을 확보하고 10~30분 정도 걸릴 것으로 예상하세요.";
"This is a very large dataset. Embedding may take 30–60 minutes and requires significant RAM." = "매우 큰 데이터셋입니다. 임베딩에 30~60분이 걸리고 많은 RAM이 필요합니다.";
"This is an extremely large dataset. Consider splitting it into smaller parts for better performance." = "매우 거대한 데이터셋입니다. 더 나은 성능을 위해 작은 부분으로 나누는 것을 고려하세요.";
"Go ahead and download! This size works well on all systems." = "바로 다운로드해도 됩니다! 이 크기는 모든 시스템에서 잘 작동합니다.";
"Recommended for most users. Make sure you have at least 4GB of free RAM." = "대부분의 사용자에게 권장. 최소 4GB의 여유 RAM을 확보하세요.";
"Recommended only if you have 8GB+ RAM available. Close other applications before embedding." = "사용 가능한 RAM이 8GB 이상일 때만 권장. 임베딩 전에 다른 앱을 종료하세요.";
"Recommended only for systems with 16GB+ RAM. Consider processing during off-hours." = "16GB 이상의 RAM이 있는 시스템에만 권장. 가능하면 업무 외 시간에 처리하세요.";
"Not recommended for typical systems. Consider finding a smaller version or subset of this dataset." = "일반적인 시스템에는 권장되지 않습니다. 더 작은 버전이나 부분집합을 고려하세요.";
"Sample dataset" = "샘플 데이터셋";
"Ready" = "준비됨";
"Open" = "열기";
"Download Dataset" = "데이터셋 다운로드";
"No files listed for this dataset." = "이 데이터셋에는 나열된 파일이 없습니다.";
"This dataset's files are not currently supported for document retrieval." = "이 데이터셋의 파일은 현재 문서 검색을 지원하지 않습니다.";
"Supported formats: %@" = "지원되는 형식: %@";
"Try another dataset if these formats aren't available." = "이 형식을 사용할 수 없으면 다른 데이터셋을 시도하세요.";
"Found unsupported: %@ …" = "지원되지 않는 항목 발견: %@ …";
"This textbook appears to be available only as a web page. Noema can't import it as a dataset." = "이 교재는 웹 페이지로만 제공되는 것 같습니다. Noema에서 데이터셋으로 가져올 수 없습니다.";
"Download complete" = "다운로드 완료";
"Downloading…" = "다운로드 중…";
"No compatible files found for retrieval. Supported formats: %@" = "검색에 사용할 수 있는 호환 파일을 찾을 수 없습니다. 지원되는 형식: %@";
"No internet connection." = "인터넷에 연결되어 있지 않습니다.";
"Request timed out. Please try again." = "요청 시간이 초과되었습니다. 다시 시도하세요.";
"Connection was lost. Please try again." = "연결이 끊어졌습니다. 다시 시도하세요.";
"Unexpected error: %@" = "예기치 못한 오류: %@";

"Model doesn't support GPU offload" = "이 모델은 GPU 오프로드를 지원하지 않습니다";
"Loading model…" = "모델 불러오는 중…";
"Select a model to load" = "불러올 모델을 선택하세요";
"Please wait" = "잠시만 기다려 주세요";
"Models Library" = "모델 라이브러리";
"Sort" = "정렬";
"Recency" = "최신순";
"Size" = "크기";
"Name" = "이름";
"Load Failed" = "불러오기 실패";
"Don't show again" = "다시 표시 안 함";

/* Noema Relay – pairing & dataset helpers */
"Model file missing (.gguf)" = "모델 파일이 없습니다 (.gguf)";
"Model path missing" = "모델 경로가 없습니다";
"Imported Dataset" = "가져온 데이터세트";
"Dataset name" = "데이터세트 이름";
"Keep this device near the Mac that's running Noema Relay to import its settings." = "설정을 가져오려면 Noema Relay가 실행 중인 Mac 근처에 이 기기를 두세요.";
"Scanning for your Mac relay…" = "Mac 릴레이를 검색하는 중…";
"Ready to scan nearby relays" = "주변 릴레이 검색 준비 완료";
"Bluetooth access is required to pair with the Mac relay." = "Mac 릴레이와 페어링하려면 Bluetooth 액세스가 필요합니다.";
"Stop Scanning" = "검색 중지";
"Start Scan" = "검색 시작";
"Connection verified. Relay details imported from this Mac." = "연결이 확인되었습니다. 이 Mac에서 릴레이 세부 정보를 가져왔습니다.";
"Signal strength unavailable" = "신호 세기를 사용할 수 없습니다";
"Very close" = "매우 가까움";
"Nearby" = "가까움";
"Within one room" = "같은 방 안";
"Move closer for a stronger signal" = "더 강한 신호를 위해 더 가까이 이동하세요.";
"This Mac" = "이 Mac";

/* Accessibility announcements */
"Model loaded." = "모델이 로드되었습니다.";
"Prompt submitted." = "프롬프트가 전송되었습니다.";
"Generating response…" = "응답을 생성하는 중…";
"Response generated." = "응답이 생성되었습니다.";

/* Tabs & accessibility labels */
"Stored" = "저장됨";
"Web Search" = "웹 검색";
"Open Stored" = "저장됨 열기";
"Message input" = "메시지 입력";
"What is Web Search button?" = "웹 검색 버튼이 무엇인가요?";

/* Mac chat quick-load menu */
"Open Model Library" = "모델 라이브러리 열기";
"Favorites" = "즐겨찾기";
"Recent" = "최근";
