/* Auto-generated localization file. */
"%@" = "%@";
"%@ %" = "%1$@ %2$";
"%@ %@" = "%1$@ %2$@";
"%@ models" = "%@ モデル";
"%@ of %@ budget" = "%2$@ 予算中 %1$@";
"%@ tokens" = "%@ トークン";
"%@ – %@" = "%1$@ – %2$@";
"%@ • %@" = "%1$@ • %2$@";
"%@%" = "%1$@%2$";
"%@% · %@" = "%1$@%2$ · %3$@";
"%@." = "%@。";
"%@:" = "%@:";
"(+ mmproj %@%)" = "(+ mmproj %1$@%2$)";
"... and %@ more" = "...そして %@ 詳細";
"320 MB • One-time download" = "320 MB • 1 回限りのダウンロード";
"320 MB • One‑time download used for local dataset search" = "320 MB • ローカル データセット検索に使用される 1 回限りのダウンロード";
"A larger context keeps more conversation history, but also uses more memory. Adjust it here." = "コンテキストが大きくなると、より多くの会話履歴が保存されますが、より多くのメモリを使用します。ここで調整してください。";
"Active" = "アクティブ";
"Active Model" = "アクティブモデル";
"Active connection via %@" = "%@ 経由のアクティブな接続";
"Active experts per token: %@ of %@" = "トークンごとのアクティブなエキスパート: %1$@/%2$@";
"Add a remote backend to configure remote startup fallbacks." = "リモート バックエンドを追加して、リモート起動フォールバックを構成します。";
"Add one or two datasets (like open textbooks) to keep responses accurate and help the AI cite sources." = "回答の正確性を維持し、AI が情報源を引用しやすくするために、1 つまたは 2 つのデータセット (公開教科書など) を追加します。";
"Add remote endpoint" = "リモートエンドポイントを追加する";
"Adjust appearance, privacy options, and network preferences here." = "ここで、外観、プライバシー オプション、ネットワーク設定を調整します。";
"Advanced" = "高度な";
"Advanced Controls" = "高度なコントロール";
"Allows models to use a privacy-preserving web search API when you tap the globe in chat. Default is ON. In Offline Only mode, the button is disabled." = "チャットで地球儀をタップしたときに、モデルがプライバシーを保護する Web 検索 API を使用できるようにします。デフォルトはオンです。オフラインのみモードでは、ボタンは無効になります。";
"Analyzing context..." = "コンテキストを分析中...";
"App Memory Usage (estimated)" = "アプリのメモリ使用量 (推定)";
"App memory usage budget: %@ (conservative)" = "アプリのメモリ使用量の予算: %@ (控えめ)";
"Approx. Tokens" = "約トークン";
"Arm web search when you truly need outside info. It has a small daily limit and most chats don’t require it." = "本当に外部情報が必要な場合は、Web 検索を強化します。 1 日あたりの制限はわずかですが、ほとんどのチャットでは制限は必要ありません。";
"Ask Noema anything" = "ノエマに何でも聞いてください";
"Ask…" = "聞く…";
"Attach Photos" = "写真を添付する";
"Backend not found" = "バックエンドが見つかりません";
"Benchmark running…" = "ベンチマーク実行中…";
"Benchmarking is not available for this model format." = "このモデル形式ではベンチマークは利用できません。";
"Blocks all network traffic, model downloads, and cloud connections so everything stays on‑device." = "すべてのネットワーク トラフィック、モデルのダウンロード、クラウド接続をブロックして、すべてがデバイス上に留まるようにします。";
"Bluetooth Pairing" = "Bluetooth ペアリング";
"Browse community models and curated datasets to expand what Noema can do." = "コミュニティ モデルと厳選されたデータセットを参照して、Noema でできることを拡張します。";
"Browse curated datasets for retrieval" = "厳選されたデータセットを参照して取得する";
"CFBundleShortVersionString1.0" = "CFBundleShortVersionString1.0";
"Cancel" = "キャンセル";
"Cancel Benchmark" = "ベンチマークをキャンセル";
"Discover Intelligence" = "インテリジェンスを発見";
"Catalog" = "カタログ";
"Chat" = "チャット";
"Chat privately with your local models, sync datasets, and manage the relay server in one place." = "ローカル モデルとプライベートにチャットし、データセットを同期し、リレー サーバーを 1 か所で管理します。";
"Chats" = "チャット";
"Checking…" = "チェック中…";
"Citation %@" = "引用 %@";
"Cloud Relay Container" = "クラウドリレーコンテナ";
"Cloud Relay via CloudKit (auto-discovery and Bluetooth pairing)" = "CloudKit経由のクラウドリレー（自動検出とBluetoothペアリング）";
"CloudKit" = "クラウドキット";
"CloudKit bridge active. Local replies are generated on this Mac." = "CloudKit ブリッジがアクティブです。ローカル応答はこの Mac 上で生成されます。";
"Compiling Metal kernels for GGUF models can take up to a minute on first load." = "GGUF モデルの Metal カーネルのコンパイルには、最初のロード時に最大 1 分かかる場合があります。";
"Complete the streaming response in the active chat before sending again." = "再度送信する前に、アクティブなチャットでストリーミング応答を完了してください。";
"Confirm and Start Embedding" = "確認して埋め込みを開始する";
"Connected" = "接続済み";
"Connection Modes" = "接続モード";
"Connection Status: %@" = "接続ステータス: %@";
"Container" = "容器";
"Context Length" = "コンテキストの長さ";
"Context Length: 4096 tokens" = "コンテキストの長さ: 4096 トークン";
"Context length is under 5000 tokens. With images and multi-sequence decoding (n_seq_max=16), per-sequence memory can be too small, leading to a crash. Increase context to at least 8192 in Model Settings." = "コンテキストの長さが 5000 トークン未満です。画像とマルチシーケンスのデコード (n_seq_max=16) では、シーケンスごとのメモリが小さすぎるため、クラッシュが発生する可能性があります。モデル設定でコンテキストを少なくとも 8192 に増やします。";
"Controls how many high‑scoring passages (chunks) can be injected into the prompt. Higher values increase recall but consume more context window and can slow responses. Typical range 3–6." = "プロンプトに挿入できる高スコアのパッセージ (チャンク) の数を制御します。値を高くするとリコールが増加しますが、コンテキスト ウィンドウの消費量が多くなり、応答が遅くなる可能性があります。通常の範囲は 3 ～ 6。";
"Couldn't load the recommended model." = "推奨モデルを読み込めませんでした。";
"Couldn’t load the recommended model right now." = "現在推奨モデルを読み込めませんでした。";
"Creativity: %@. Low values focus responses; high values add variety." = "創造性: %@。値を低くすると、応答が集中します。値を高くすると多様性が増します。";
"Dark" = "暗い";
"Dataset" = "データセット";
"Dataset indexing in progress..." = "データセットのインデックス作成が進行中です...";
"Dataset ready to use" = "すぐに使えるデータセット";
"Datasets" = "データセット";
"Datasets enrich the model with focused knowledge. Toggle one on to use it in chat." = "データセットは、焦点を絞った知識でモデルを強化します。チャットで使用するには、1 つをオンに切り替えます。";
"Default selection (~%@) balances RAM usage against model quality." = "デフォルトの選択 (~%@) では、RAM の使用量とモデルの品質のバランスがとれます。";
"Delete" = "消去";
"Delete Dataset" = "データセットの削除";
"Deletes all chats, downloaded models, and datasets, and restores settings to defaults. The embedding model stays installed." = "すべてのチャット、ダウンロードしたモデル、データセットを削除し、設定をデフォルトに戻します。埋め込みモデルはインストールされたままになります。";
"Device" = "デバイス";
"Digest:" = "ダイジェスト：";
"Done" = "終わり";
"Done!" = "終わり！";
"Download Now" = "今すぐダウンロード";
"Download a model from Explore or add a remote endpoint to get started." = "Explore からモデルをダウンロードするか、リモート エンドポイントを追加して開始します。";
"Download a small embedding model so Noema can index and search your datasets" = "Noema がデータセットのインデックスを作成して検索できるように、小さな埋め込みモデルをダウンロードします。";
"Downloaded datasets need on-device embedding. Give it a few minutes after download finishes." = "ダウンロードしたデータセットはデバイス上で埋め込む必要があります。ダウンロードが完了してから数分待ちます。";
"Downloaded models and datasets live here so you can manage them offline." = "ダウンロードしたモデルとデータセットはここに保存されるため、オフラインで管理できます。";
"Downloading…" = "ダウンロード中…";
"Draft tokens: %@" = "ドラフトトークン: %@";
"Draft window: %@" = "ドラフトウィンドウ: %@";
"EPUB viewing not supported on this platform" = "このプラットフォームでは EPUB の表示がサポートされていません";
"Embedding" = "埋め込み";
"Embedding Model Ready" = "埋め込みモデルの準備完了";
"Embedding is resource intensive. For best performance, plug in your phone. Do you want to proceed on battery?" = "埋め込みはリソースを大量に消費します。最高のパフォーマンスを得るには、携帯電話を接続してください。バッテリーで続行しますか?";
"Enabling Bluetooth…" = "Bluetooth を有効にしています…";
"Enhance with Datasets" = "データセットによる強化";
"Error" = "エラー";
"Estimated working set: %@ · Budget: %@" = "推定ワーキングセット: %1$@ · 予算: %2$@";
"Experts Per Token" = "トークンごとのエキスパート";
"Explore Datasets" = "データセットを探索する";
"Expose any downloaded models or connected remote endpoints from the Stored tab to your paired devices. Select which one should answer conversations when the relay is running." = "ダウンロードしたモデルまたは接続されたリモート エンドポイントを [保存済み] タブからペアリングされたデバイスに公開します。リレーの実行中にどちらが会話に応答するかを選択します。";
"Expose to iOS" = "iOS に公開する";
"Explore the latest open-source models optimized for your Mac." = "Mac に最適化された最新のオープンソースモデルを探索しましょう。";
"Failed to load README" = "READMEのロードに失敗しました";
"Failed: %@" = "失敗しました: %@";
"Fastest option on this device: SLM (Leap) models." = "このデバイスの最速オプション: SLM (Leap) モデル。";
"Field requirements will depend on your specific backend deployment." = "フィールドの要件は、特定のバックエンドの展開によって異なります。";
"Files" = "ファイル";
"First, enable fast dataset search" = "まず、高速データセット検索を有効にします";
"First-time GGUF load takes longer" = "初回の GGUF ロードには時間がかかります";
"First-time download from HuggingFace" = "HuggingFace からの初回ダウンロード";
"First‑time setup: download the Qwen‑1.7B model and embeddings.\nWi‑Fi recommended." = "初回セットアップ: Qwen-1.7B モデルと埋め込みをダウンロードします。\nWi-Fi を推奨します。";
"For best performance, please plug in your phone until this completes." = "最高のパフォーマンスを得るには、これが完了するまで携帯電話を接続してください。";
"Force Local Network" = "ローカルネットワークを強制する";
"Forces chat traffic through the last LAN host even if Wi‑Fi names don't match yet." = "Wi-Fi 名がまだ一致していない場合でも、チャット トラフィックが最後の LAN ホストを強制的に通過します。";
"Formatted view unavailable" = "フォーマットされたビューは使用できません";
"Found unsupported: %@ …" = "サポートされていないことが見つかりました: %@ …";
"Frequency penalty: %@" = "周波数ペナルティ: %@";
"GGUF models are the most compatible option. Use the format switch to explore the other builds when you need them." = "GGUF モデルは最も互換性のあるオプションです。必要に応じて、形式スイッチを使用して他のビルドを探索します。";
"GGUF works everywhere. MLX targets Apple Silicon speed. SLM focuses on responsiveness on any device." = "GGUF はどこでも機能します。 MLX は Apple Silicon の速度をターゲットとしています。 SLM は、あらゆるデバイスでの応答性に重点を置いています。";
"GPU Offload Layers" = "GPU オフロード レイヤー";
"GPU off-load is not supported for this model." = "このモデルでは GPU オフロードはサポートされていません。";
"Get Started" = "始めましょう";
"Help shape Noema by trying upcoming features and sharing feedback." = "今後の機能を試したり、フィードバックを共有したりして、Noema の形成に協力してください。";
"High context lengths use more memory" = "コンテキストの長さが長いと、より多くのメモリを使用します";
"High-quality embedding model for local RAG" = "ローカル RAG の高品質埋め込みモデル";
"Host ID: %@" = "ホストID: %@";
"How it works" = "仕組み";
"I'm New to Local LLMs, Guide Me" = "ローカル LLM は初めてです。ガイドしてください";
"If enabled, the app will attempt to load models even when they likely exceed your device's memory budget. This can cause the app to terminate." = "有効にすると、アプリは、デバイスのメモリ バジェットを超える可能性がある場合でもモデルの読み込みを試みます。これにより、アプリが終了する可能性があります。";
"Import Dataset" = "データセットのインポート";
"Import PDFs, EPUBs, or text files to build local knowledge bases." = "PDF、EPUB、またはテキスト ファイルをインポートして、ローカルのナレッジ ベースを構築します。";
"Import your own PDFs, EPUBs, or TXT files and keep them local." = "独自の PDF、EPUB、または TXT ファイルをインポートし、ローカルに保存します。";
"Importing & Scanning..." = "インポートとスキャン中...";
"In progress..." = "進行中...";
"Indexing %@" = "%@ のインデックス作成";
"Indexing dataset…" = "データセットのインデックス作成中…";
"Indexing: %@% · %@" = "インデックス作成: %1$@%2$ · %3$@";
"Install a local model to make it available at launch." = "ローカル モデルをインストールして、起動時に使用できるようにします。";
"Install another model with the same architecture and equal or smaller size to enable speculative decoding." = "投機的デコードを可能にするには、同じアーキテクチャで同じかそれより小さいサイズの別のモデルをインストールします。";
"K Cache Quant" = "K キャッシュ量";
"Keep this iPhone or iPad within a few feet of the Mac that is advertising Noema Relay. We'll pull the relay details automatically once connected." = "この iPhone または iPad を、Noema Relay を宣伝している Mac から数フィート以内に置いてください。接続されると、リレーの詳細が自動的に取得されます。";
"LAN URL: %@" = "LAN URL: %@";
"Large Model Downloads" = "大規模なモデルのダウンロード";
"Last Sync" = "最終同期";
"Last refreshed %@" = "最終更新日 %@";
"Latest benchmark" = "最新のベンチマーク";
"Latest integrated release: %@" = "最新の統合リリース: %@";
"Library" = "図書館";
"Light" = "ライト";
"Llama.cpp" = "Call.cpp";
"Load" = "負荷";
"Load a local model before chatting. You can download one from the Explore tab or load a model you've already installed." = "チャットする前にローカル モデルをロードします。 [Explore] タブからモデルをダウンロードすることも、すでにインストールされているモデルをロードすることもできます。";
"Loading recommendation…" = "推奨事項を読み込んでいます…";
"Loading…" = "読み込み中…";
"Local Network HTTP server for LAN clients (OpenAI-compatible)" = "LANクライアント用ローカルネットワークHTTPサーバー（OpenAI互換）";
"Low = focused. High = varied." = "低い = 集中力。高い = 多様です。";
"Lower = more results (more noise). Higher = stricter matches." = "低い = 結果が多くなります (ノイズが多くなります)。高い = より厳密な一致。";
"MLX currently manages expert routing automatically; manual selection is not supported." = "MLX は現在、エキスパート ルーティングを自動的に管理します。手動選択はサポートされていません。";
"Many models are several gigabytes in size and require a stable connection and sufficient storage. Downloads can fail or take a long time on slow networks or devices with limited space." = "多くのモデルのサイズは数ギガバイトであり、安定した接続と十分なストレージが必要です。低速なネットワークやスペースが限られているデバイスでは、ダウンロードが失敗したり、長時間かかる場合があります。";
"Max Chunks: %@" = "最大チャンク: %@";
"Max recommended context on this device: ~%@ tokens" = "このデバイスで推奨される最大コンテキスト: ~%@ トークン";
"Measure real-world generation speed for this configuration. A short scripted prompt will run locally and report timing and memory usage." = "この構成の実際の生成速度を測定します。短いスクリプト化されたプロンプトがローカルで実行され、タイミングとメモリ使用量がレポートされます。";
"Min-p" = "ミンプ";
"Min-p: %@" = "ミンプ: %@";
"Minimum cosine similarity a passage must have to be considered relevant. Lower = more passages (higher recall, more noise). Higher = fewer, more precise passages. Try 0.2–0.4 for broad questions; 0.5–0.7 for precise lookups." = "関連性があると見なされるには、パッセージの最小コサイン類似度が必要です。低い = パッセージが多くなります (再現率が高く、ノイズが多くなります)。高い = パッセージが少なく、より正確になります。幅広い質問には 0.2 ～ 0.4 を試してください。正確な検索の場合は 0.5 ～ 0.7。";
"MoE layers: %@ / %@" = "MoE レイヤー: %1$@ / %2$@";
"Model Detection Limitations" = "モデル検出の制限";
"Model Formats" = "モデル形式";
"Models" = "モデル";
"Models shown here are exposed by the Mac relay. Manage sources in the Relay tab on macOS to share more models." = "ここに示されているモデルは Mac リレーによって公開されています。 macOS の [リレー] タブでソースを管理して、より多くのモデルを共有します。";
"Move your device closer to the Mac running the relay if it doesn't appear right away. Bluetooth discovery usually completes within a few seconds." = "すぐに表示されない場合は、リレーを実行している Mac にデバイスを近づけます。 Bluetooth の検出は通常、数秒以内に完了します。";
"Name your dataset" = "データセットに名前を付けます";
"Nearby Relays" = "近くのリレー";
"Nearby iPhone and iPad devices discover your Mac relay instantly and sync pairing codes over the air." = "近くの iPhone および iPad デバイスは Mac リレーを即座に検出し、ペアリング コードを無線で同期します。";
"Need a fresh thread? Tap the plus button for a brand-new conversation." = "新しいスレッドが必要ですか?プラスボタンをタップすると、まったく新しい会話が始まります。";
"Nice! You already have the recommended GGUF starter model ready to use." = "ニース！推奨される GGUF スターター モデルはすでに用意されており、すぐに使用できます。";
"No compatible files found for retrieval. Supported: PDF, EPUB, TXT, MD, JSON, JSONL, CSV, TSV" = "取得できる互換性のあるファイルが見つかりません。サポートされている: PDF、EPUB、TXT、MD、JSON、JSONL、CSV、TSV";
"No connection responses recorded yet." = "接続応答はまだ記録されていません。";
"No datasets available yet. Import or download datasets to build your personal library." = "利用可能なデータセットはまだありません。データセットをインポートまたはダウンロードして、個人ライブラリを構築します。";
"No datasets found. Try different keywords." = "データセットが見つかりません。別のキーワードを試してください。";
"No datasets imported yet." = "データセットはまだインポートされていません。";
"No datasets yet" = "データセットはまだありません";
"No files listed for this dataset." = "このデータセットにはファイルがリストされていません。";
"No model >" = "モデルなし >";
"No model loaded" = "モデルがロードされていません";
"No models available. Add downloads or remote connections in Stored to configure the relay." = "利用可能なモデルはありません。 Stored にダウンロードまたはリモート接続を追加して、リレーを構成します。";
"No models cached yet. Open the backend to refresh its catalog." = "まだキャッシュされたモデルはありません。バックエンドを開いてカタログを更新します。";
"No models found for '%@'" = "「%@」のモデルが見つかりませんでした";
"No models loaded right now. We'll spin one up when a request arrives." = "現在ロードされているモデルはありません。リクエストが到着したらスピンアップします。";
"No models match your search." = "検索に一致するモデルはありません。";
"No models yet" = "まだモデルがありません";
"No parameters" = "パラメータなし";
"No quant files available" = "利用可能なクォント ファイルがありません";
"No recent devices. We'll list clients the next time they talk to this relay." = "最近のデバイスはありません。クライアントが次回このリレーと通信するときに、クライアントをリストします。";
"No remote endpoints configured yet." = "リモート エンドポイントはまだ構成されていません。";
"No remote endpoints configured." = "リモート エンドポイントが構成されていません。";
"No ≥Q3 quants are available for this model." = "このモデルでは ≥Q3 クォントは使用できません。";
"Noema" = "11月";
"Noema REST API — /api/v0/* for model catalog & operations" = "Noema REST API — /api/v0/* モデル カタログと操作用";
"Noema Relay" = "ノエマリレー";
"Noema Server" = "ノエマサーバー";
"Noema attempts to gauge available memory to prevent models from exceeding device limits. These checks may occasionally miss risky situations and allow a model to crash your app, or they may be overly conservative and block a model that could have run fine." = "Noema は、モデルがデバイスの制限を超えないようにするために、利用可能なメモリを測定しようとします。これらのチェックでは、場合によっては危険な状況を見逃してモデルがアプリをクラッシュさせたり、過度に保守的で正常に実行できたはずのモデルをブロックしたりする場合があります。";
"Noema could not find a projector in the repository. If the model advertises vision, ensure the mmproj file is present in the same folder as the weights." = "Noema はリポジトリ内でプロジェクターを見つけることができませんでした。モデルがビジョンをアドバタイズする場合は、mmproj ファイルがウェイトと同じフォルダーに存在することを確認してください。";
"Noema has been reset. The embedding model remains installed." = "ノエマがリセットされました。埋め込みモデルはインストールされたままになります。";
"Nomic Embed Text v1.5 (Q4_K_M)" = "Nomic 埋め込みテキスト v1.5 (Q4_K_M)";
"None" = "なし";
"Not found" = "見つかりません";
"Not provided" = "提供されません";
"OK" = "わかりました";
"Off-Grid" = "オフグリッド";
"Off-grid mode blocks every network call so the app stays self-contained. Good luck exploring Noema!" = "オフグリッド モードではすべてのネットワーク呼び出しがブロックされるため、アプリは自己完結型を維持します。ノエマの探検頑張ってください!";
"Only one expert is available for this model; the active expert count is fixed." = "このモデルに対応できる専門家は 1 人だけです。アクティブなエキスパートの数は固定されています。";
"Open Stored to choose a model to run locally or connect to a remote endpoint." = "Stored を開いて、ローカルで実行するモデルを選択するか、リモート エンドポイントに接続します。";
"Open the sidebar to revisit any previous session without losing your spot." = "サイドバーを開いて、その場を失うことなく以前のセッションに再度アクセスしてください。";
"OpenAI-style API — /v1/chat/completions, /v1/completions, /v1/models" = "OpenAI スタイルの API — /v1/chat/completions、/v1/completions、/v1/models";
"Optimizations in use" = "使用中の最適化";
"PDF viewing not supported on this platform" = "このプラットフォームでは PDF の表示がサポートされていません";
"Pick a model and add a dataset" = "モデルを選択してデータセットを追加する";
"Pick the SLM format when you want ultra-responsive models that run well anywhere." = "どこでも快適に動作する超応答性の高いモデルが必要な場合は、SLM 形式を選択してください。";
"Pinned answer" = "固定された回答";
"Pinned answer unavailable" = "固定された回答は利用できません";
"Preparation" = "準備";
"Preparing Embedding Model" = "埋め込みモデルの準備";
"Preparing benchmark…" = "ベンチマークを準備しています…";
"Preparing…" = "準備中…";
"Presence penalty: %@" = "プレゼンスペナルティ: %@";
"Projector (mmproj)" = "プロジェクター（mmproj）";
"Projector downloaded automatically from Hugging Face. Keep this file alongside the weights so vision remains available." = "プロジェクターは Hugging Face から自動的にダウンロードされました。視覚を利用できるように、このファイルをウェイトと一緒に保管してください。";
"Quantize the runtime key cache to save memory. Experimental." = "ランタイムキーキャッシュを量子化してメモリを節約します。実験的。";
"Quantize the runtime value cache to save memory when Flash Attention is enabled. Experimental." = "Flash アテンションが有効な場合、ランタイム値キャッシュを量子化してメモリを節約します。実験的。";
"Qwen 3 1.7B GGUF (Q3_K_M) gives you a dependable starting point. Delete it anytime if you need space." = "Qwen 3 1.7B GGUF (Q3_K_M) は、信頼できる出発点を提供します。スペースが必要な場合はいつでも削除してください。";
"RAG embeds normalized paragraphs from your PDFs and EPUBs. On each question, the most relevant chunks are retrieved and added to the prompt. Images are ignored." = "RAG は、PDF および EPUB から正規化された段落を埋め込みます。質問ごとに、最も関連性の高いチャンクが取得され、プロンプトに追加されます。画像は無視されます。";
"RAM Safety Checks" = "RAMの安全性チェック";
"RAM information for this device will be added in a future update." = "このデバイスの RAM 情報は将来のアップデートで追加される予定です。";
"REST Endpoints" = "RESTエンドポイント";
"Reachable at" = "到達可能な場所";
"Ready for Use" = "すぐに使用可能";
"Recent Chats" = "最近のチャット";
"Recommended" = "推奨";
"Recommended Starter Model" = "スターター推奨モデル";
"Relay ID" = "リレーID";
"Relay Server Running" = "リレーサーバーの実行中";
"Relay Sources" = "リレーソース";
"Remaining: %@" = "残り: %@";
"Remember:" = "覚えて：";
"Remote Backends" = "リモートバックエンド";
"Remote endpoint is offline. This model can't be found at this time." = "リモート エンドポイントがオフラインです。このモデルは現時点では見つかりません。";
"Remote timeout: %@s" = "リモートタイムアウト: %@s";
"Repeat last N tokens: %@" = "最後の N 個のトークンを繰り返す: %@";
"Repetition penalty: %@" = "繰り返しペナルティ: %@";
"Request Parameters" = "リクエストパラメータ";
"Responses are generated by the macOS relay server. Configure the provider (LM Studio or Ollama) on the Mac app." = "応答は macOS リレー サーバーによって生成されます。 Mac アプリでプロバイダー (LM Studio または Ollama) を構成します。";
"Result" = "結果";
"Results" = "結果";
"Save" = "保存";
"Saving…" = "保存中…";
"Score: %@" = "スコア: %@";
"SearXNG web search is available without limits. There's nothing to purchase—just enable the globe button in chat whenever you need online results." = "SearXNG Web 検索は制限なく利用できます。購入するものは何もありません。オンラインの結果が必要な場合は、チャット内の地球儀ボタンを有効にするだけです。";
"SearXNG web search is enabled for this device." = "このデバイスでは SearXNG Web 検索が有効になっています。";
"Search" = "検索";
"Search for any subject you're interested in." = "興味のあるテーマを検索してください。";
"Search requests are proxied through https://search.noemaai.com and are available without quotas." = "検索リクエストは https://search.noemaai.com を通じてプロキシされ、割り当てなしで利用できます。";
"Seed" = "シード";
"Selecting more experts keeps additional expert weights resident in RAM and increases memory usage." = "さらに多くのエキスパートを選択すると、追加のエキスパートの重みが RAM に常駐し、メモリ使用量が増加します。";
"Server Settings" = "サーバー設定";
"Share Logs" = "ログを共有する";
"Sharing relay payload with nearby devices…" = "近くのデバイスとリレー ペイロードを共有しています…";
"Shows the last server response." = "最後のサーバー応答を表示します。";
"Signal" = "信号";
"Similarity Threshold" = "類似性のしきい値";
"Simple" = "単純";
"Smooth loops and phrase echo by balancing repetition controls." = "繰り返しコントロールのバランスを整えることで、ループやフレーズのエコーをスムーズにします。";
"Smooth loops and repeated phrases by tuning repetition controls." = "繰り返しコントロールを調整することで、ループや繰り返されるフレーズをスムーズにします。";
"Some models do not provide the system prompts needed for Noema to detect and configure them properly. These models may be unusable until they include appropriate metadata or support." = "一部のモデルでは、Noema がモデルを適切に検出して構成するために必要なシステム プロンプトが提供されません。これらのモデルは、適切なメタデータまたはサポートが含まれるまで使用できない場合があります。";
"Source unavailable. Check storage or network settings." = "ソースが利用できません。ストレージまたはネットワークの設定を確認してください。";
"Source: %@" = "出典: %@";
"Specify identifiers for models that are not listed by the server. Leave blank to rely on the server's catalog." = "サーバーによってリストされていないモデルの識別子を指定します。サーバーのカタログに依存する場合は、空白のままにします。";
"Specify your model identifiers, or reload your custom models later." = "モデル識別子を指定するか、後でカスタム モデルをリロードします。";
"Speed up with a smaller helper model." = "より小さなヘルパー モデルを使用して高速化します。";
"Start by installing one model. Then add a dataset (like an open textbook) so the AI can answer with grounded knowledge." = "まず 1 つのモデルをインストールします。次に、AI が根拠のある知識に基づいて回答できるように、データセット (公開教科書のような) を追加します。";
"Start the relay to automatically share the latest payload with nearby devices." = "リレーを開始して、近くのデバイスと最新のペイロードを自動的に共有します。";
"Start with a reliable Qwen 3 1.7B build. It balances capability with small download size." = "信頼性の高い Qwen 3 1.7B ビルドから始めます。機能と小さいダウンロード サイズのバランスが取れています。";
"Startup defaults now live in Settings → Startup. Favorite models here to keep them handy." = "スタートアップのデフォルトが [設定] → [スタートアップ] に表示されるようになりました。お気に入りのモデルを手元に置いておけます。";
"Status" = "状態";
"Stay close to your Mac" = "Mac の近くにいてください";
"Stop Using Dataset" = "データセットの使用を停止する";
"Streaming" = "ストリーミング";
"Streaming response…" = "ストリーミング応答…";
"Supported Endpoints" = "サポートされているエンドポイント";
"Supported formats: PDF, EPUB, TXT, MD, JSON, JSONL, CSV, TSV" = "サポートされている形式: PDF、EPUB、TXT、MD、JSON、JSONL、CSV、TSV";
"Swap between the new stacked chat panel and the classic tab bar layout." = "新しいスタック型チャット パネルとクラシック タブ バー レイアウトを切り替えます。";
"Swipe left to remove the embedding model from this device." = "左にスワイプして、このデバイスから埋め込みモデルを削除します。";
"Switch the selector to MLX for Apple Silicon‑optimized builds that excel at speed." = "セレクターを MLX に切り替えて、速度に優れた Apple Silicon に最適化されたビルドを実現します。";
"Switch to Raw to inspect the original response." = "Raw に切り替えて、元の応答を検査します。";
"System" = "システム";
"Tap to load" = "タップしてロード";
"Temperature" = "温度";
"Testing" = "テスト";
"The app memory usage budget is an estimate based on your device's total RAM and typical iOS memory management. The actual available memory may vary depending on system load, other running apps, and iOS memory pressure. Models that exceed this budget may cause the app to be terminated by iOS." = "アプリのメモリ使用量の予算は、デバイスの合計 RAM と一般的な iOS メモリ管理に基づいた推定値です。実際に利用可能なメモリは、システム負荷、実行中の他のアプリ、iOS のメモリ負荷によって異なる場合があります。この予算を超えるモデルは、iOS によってアプリが終了される可能性があります。";
"The embedding model is installed. Delete it to free ~320 MB." = "埋め込みモデルがインストールされます。それを削除すると、最大 320 MB が解放されます。";
"The relay listens to the %@ container for new conversations and responds with your selected provider." = "リレーは %@ コンテナで新しい会話をリッスンし、選択したプロバイダーで応答します。";
"The tool returned data that can't be formatted. Switch to Raw to inspect the original response." = "ツールはフォーマットできないデータを返しました。 Raw に切り替えて、元の応答を検査します。";
"These options stay in simple mode for clarity. Let’s cover the essentials." = "わかりやすくするために、これらのオプションはシンプル モードのままです。要点を説明しましょう。";
"Think of Noema as a simple way to run AI on your device. To get useful answers, you pair a local model with datasets (like open textbooks). We’ll guide you through the first setup." = "Noema は、デバイス上で AI を実行する簡単な方法だと考えてください。有用な答えを得るには、ローカル モデルとデータセット (開かれた教科書など) を組み合わせます。最初のセットアップについて説明します。";
"This app bundles llama.cpp; we keep this in sync with upstream b‑releases." = "このアプリには llama.cpp がバンドルされています。これをアップストリームの b-release と同期させます。";
"This backend is unavailable. Remove it or pick another option." = "このバックエンドは使用できません。それを削除するか、別のオプションを選択してください。";
"This chat stays private—responses are generated on your device after you load a model." = "このチャットは非公開のままです。モデルをロードした後、デバイス上で応答が生成されます。";
"This configuration exceeds the current RAM safety guard, so benchmarking is disabled." = "この構成は現在の RAM 安全保護を超えているため、ベンチマークは無効になっています。";
"This dataset is taking a while to load, still working…" = "このデータセットはロードに時間がかかっていますが、まだ動作しています…";
"This dataset's files are not currently supported for document retrieval." = "このデータセットのファイルは現在、ドキュメントの取得をサポートしていません。";
"This device doesn't support GPU offload." = "このデバイスは GPU オフロードをサポートしていません。";
"This device doesn't support GPU offload; GGUF models will run on the CPU and generation speed will be significantly slower." = "このデバイスは GPU オフロードをサポートしていません。 GGUF モデルは CPU 上で実行され、生成速度は大幅に遅くなります。";
"This model doesn't support GPU offload and generation speed will be significantly slower. Consider switching to an MLX model." = "このモデルは GPU オフロードをサポートしていないため、生成速度が大幅に遅くなります。 MLX モデルへの切り替えを検討してください。";
"This model doesn't support GPU offload and generation speed will be significantly slower. Fastest option on this device: use an SLM (Leap) model." = "このモデルは GPU オフロードをサポートしていないため、生成速度が大幅に遅くなります。このデバイスの最速オプション: SLM (Leap) モデルを使用します。";
"This model doesn't support GPU offload and may run slowly. Consider an MLX model." = "このモデルは GPU オフロードをサポートしていないため、動作が遅くなる可能性があります。 MLX モデルを考えてみましょう。";
"This model doesn't support GPU offload and may run slowly. Fastest option: use an SLM model." = "このモデルは GPU オフロードをサポートしていないため、動作が遅くなる可能性があります。最速のオプション: SLM モデルを使用します。";
"This permanently removes every chat conversation. This action cannot be undone." = "これにより、すべてのチャット会話が完全に削除されます。この操作は元に戻すことができません。";
"This textbook appears to be available only as a web page. Noema can't import it as a dataset." = "この教科書はウェブページとしてのみ入手可能のようです。 Noema はデータセットとしてインポートできません。";
"Tool calling isn't perfect. Although Noema implements many methods of detecting and instructing models to use tools, not all LLMs will follow instructions and some might not call them correctly or at all. Tool calling heavily depends on model pre-training and will get better as time passes." = "ツールの呼び出しは完璧ではありません。 Noema はモデルを検出してツールの使用を指示する多くのメソッドを実装していますが、すべての LLM が指示に従うわけではなく、一部の LLM は正しく呼び出されないか、まったく呼び出されない可能性があります。ツールの呼び出しはモデルの事前トレーニングに大きく依存しており、時間の経過とともに改善されます。";
"Tools" = "ツール";
"Top-k: %@" = "トップ-k: %@";
"Top-p" = "トップ";
"Top-p: %@" = "トップ: %@";
"Try again" = "もう一度やり直してください";
"Try another dataset if these formats aren't available." = "これらの形式が利用できない場合は、別のデータセットを試してください。";
"Try the Qwen 3 1.7B GGUF (Q3_K_M) build below. It's a good starting point and you can delete it anytime." = "以下の Qwen 3 1.7B GGUF (Q3_K_M) ビルドを試してください。これは出発点として適しており、いつでも削除できます。";
"Unable to load image" = "画像をロードできません";
"Unknown error" = "不明なエラー";
"Updates every second" = "毎秒更新";
"Use" = "使用";
"Use Dataset" = "データセットの使用";
"Use this switch to flip between finding models or datasets." = "このスイッチを使用して、モデルまたはデータセットの検索を切り替えます。";
"Using %@" = "%@ の使用";
"Using %@ of %@ budget" = "%2$@ 予算中 %1$@ を使用";
"Using more than %@ significantly increases RAM usage." = "%@ を超えるものを使用すると、RAM の使用量が大幅に増加します。";
"Using server catalog" = "サーバーカタログの使用";
"V Cache Quant" = "Vキャッシュ量";
"Vendor recommendation: %@" = "ベンダーの推奨: %@";
"Version" = "バージョン";
"Version %@" = "バージョン %@";
"Version 1.4" = "バージョン1.4";
"Vision models require a companion projector (.mmproj). Noema will fetch it automatically the next time you download this model." = "ビジョン モデルにはコンパニオン プロジェクター (.mmproj) が必要です。次回このモデルをダウンロードするときに、Noema が自動的に取得します。";
"Wait for the response in your other chat to finish before sending a new message." = "新しいメッセージを送信する前に、他のチャットの応答が完了するまで待ってください。";
"Waiting for tool response…" = "ツールの応答を待っています…";
"We'll extract text and prepare embeddings. You can also start later from the dataset details." = "テキストを抽出して埋め込みを準備します。後でデータセットの詳細から開始することもできます。";
"We'll route new conversations through %@ even if Wi‑Fi names differ. You can switch back by reloading the backend." = "Wi‑Fi 名が異なる場合でも、新しい会話は %@ 経由でルーティングされます。バックエンドをリロードすることで元に戻すことができます。";
"We'll try remote models in priority order for this long before moving to the next option." = "次のオプションに進む前に、この期間、優先順位に従ってリモート モデルを試してみます。";
"We'll try this saved identifier even though it's not in the latest catalog." = "最新のカタログにない場合でも、この保存された識別子を試してみます。";
"Web Search Tool Calls" = "Web 検索ツールの呼び出し";
"Web Search button" = "ウェブ検索ボタン";
"Web search is included" = "Web検索が含まれています";
"Weights" = "重み";
"Welcome to Noema" = "ノエマへようこそ";
"Welcome to Noema for Mac" = "Noema for Mac へようこそ";
"What is Noema?" = "ノエマって何？";
"When connecting from another device, point the base URL to your computer (for example http://192.168.0.10:11434) and start Ollama with `OLLAMA_HOST=0.0.0.0` so it accepts remote clients." = "別のデバイスから接続する場合は、ベース URL をコンピュータ (例: http://192.168.0.10:11434) に指定し、「OLLAMA_HOST=0.0.0.0」で Ollama を起動し、リモート クライアントを受け入れます。";
"When enabled, pressing eject on iOS tells this Mac to unload the active relay model." = "有効にすると、iOS でイジェクトを押すと、この Mac にアクティブなリレー モデルをアンロードするように指示されます。";
"When enabled, you will be asked to choose parameters every time a model loads." = "有効にすると、モデルをロードするたびにパラメーターを選択するように求められます。";
"Wi-Fi: %@" = "WiFi: %@";
"Working set estimate (%@): %@ @ %@ tokens" = "ワーキングセットの見積もり (%1$@): %2$@ @ %3$@ トークン";
"Write prompts, instructions, or notes here. Press return to add new lines." = "ここにプロンプ​​ト、指示、またはメモを書きます。新しい行を追加するには、Return キーを押します。";
"You can keep chatting while indexing finishes" = "インデックス作成が完了している間もチャットを続けることができます";
"You can only favorite up to three models." = "お気に入りに登録できるモデルは 3 つまでです。";
"You can restart this process in the dataset settings any time." = "このプロセスは、データセット設定でいつでも再開できます。";
"You're in Off-Grid mode. The Explore tab is hidden and all network features are disabled. You can only use downloaded models and datasets." = "オフグリッド モードになっています。 [探索] タブが非表示になり、すべてのネットワーク機能が無効になります。ダウンロードしたモデルとデータセットのみを使用できます。";
"Your Datasets" = "あなたのデータセット";
"Your Models" = "あなたのモデル";
"Your private AI workspace" = "あなたのプライベート AI ワークスペース";
"You’re ready to explore. Download models, add datasets, and start chatting." = "探検する準備ができました。モデルをダウンロードし、データセットを追加し、チャットを開始します。";
"minutes" = "分";
"•" = "";
"• Close other applications to free up RAM" = "• 他のアプリケーションを閉じて RAM を解放します。";
"• Embedding happens locally on your device" = "• 埋め込みはデバイス上でローカルに行われます";
"• Larger datasets take exponentially more time" = "• データセットが大きくなると、飛躍的に時間がかかります";
"• You can pause and resume downloads if needed" = "• 必要に応じてダウンロードを一時停止したり再開したりできます。";
"…and %@ more parameter%@" = "…そして %1$@ さらなるパラメータ%2$@";
"⚠️ " = "";
"General" = "一般";

"Privacy" = "プライバシー";

"About" = "について";

"About & Support" = "について・サポート";

"Network" = "ネットワーク";

"Embedding Model" = "埋め込みモデル";

"Retrieval" = "リトリーバル";

"Early Testers" = "早期テスター";

"Build Info" = "ビルド情報";

"Settings" = "設定";

"Language" = "言語";
"Startup" = "起動";
"Search models" = "モデルを検索";
"SLM Models - Liquid AI" = "SLM Models - Liquid AI";
"Import" = "インポート";
"Import GGUF" = "GGUF をインポート";
"Import MLX" = "MLX をインポート";
"Import Failed" = "インポートに失敗しました";
"Switching between GGUF/MLX modes" = "GGUF/MLX モードを切り替える";
"Switching between GGUF/SLM modes" = "GGUF/SLM モードを切り替える";
"Vision-capable model" = "ビジョン対応モデル";
"All" = "すべて";
"Text" = "テキスト";
"Vision" = "ビジョン";
"Continue" = "続行";
"Try bullet" = "次を試してください:\n• 別のキーワード（例: “gemma 3” の代わりに “gemma-3”）\n• %@\n• テキスト/ビジョン フィルターを調整\n• 検索フィルターを確認";
"Select one or more .gguf files. For vision models, also select the projector .gguf (files containing ‘mmproj’ or ‘projector’). Files will be copied into your local model library." = "Select one or more .gguf files. For vision models, also select the projector .gguf (files containing ‘mmproj’ or ‘projector’). Files will be copied into your local model library.";
"Select the model folder that contains config.json and the weights (safetensors/npz). The entire folder will be imported into your local model library." = "Select the model folder that contains config.json and the weights (safetensors/npz). The entire folder will be imported into your local model library.";
"K Cache Quantization" = "Kキャッシュ量子化";
"V Cache Quantization" = "Vキャッシュ量子化";
"Favorite Limit Reached" = "お気に入り上限に達しました";
"Overview" = "概要";
"Sampling" = "サンプリング";
"Speculative Decoding" = "推測デコーディング";
"Benchmark" = "ベンチマーク";
"Maintenance" = "メンテナンス";
"MLX" = "MLX";
"Tokenizer Path (tokenizer.json)" = "トークナイザーのパス (tokenizer.json)";
"Back" = "戻る";
"Favorite Model" = "お気に入りモデル";
"Reset to Default Settings" = "デフォルト設定にリセット";
"Delete Model" = "モデルを削除";
"Delete %@?" = "%@ を削除しますか？";
"Not provided by repository" = "リポジトリに含まれていません";
"Unknown (not checked yet)" = "不明（未確認）";
"Keep Model In Memory" = "モデルをメモリに保持";
"GPU Offload Layers: %@/%@" = "GPUオフロード層: %1$@/%2$@";
"CPU Threads: %@" = "CPUスレッド: %@";
"Offload KV Cache to GPU" = "KVキャッシュをGPUにオフロード";
"Use mmap()" = "mmap() を使用";
"Random" = "ランダム";
"Flash Attention" = "Flash Attention";
"1 expert" = "専門家1人";
"%@ experts" = "%@ 人の専門家";
"Helper Model" = "補助モデル";
"Draft strategy" = "ドラフト戦略";
"Run Benchmark" = "ベンチマークを実行";
"Benchmarking…" = "ベンチマーク中…";
"Leap SLM models manage runtime optimizations automatically." = "Leap SLM モデルは実行時の最適化を自動管理します。";
"This format doesn't expose tunable runtime optimizations." = "この形式では調整可能な実行時最適化は提供されません。";
"Token processing" = "トークン処理";
"Token generation" = "トークン生成";
"Total time" = "合計時間";
"First token" = "最初のトークン";
"Peak memory" = "ピークメモリ";
"Output tokens" = "出力トークン";
"End Guide" = "ガイドを終了";
"Streaming benchmark output…" = "ベンチマーク出力をストリーミング中…";
"Streaming… %@ chunks (~%@ tok est.)" = "ストリーミング中… %1$@ チャンク（約 %2$@ トークン推定）";
"Streaming… %d chunks (~%d tok est.)" = "ストリーミング中… %d チャンク（約 %d トークン推定）";
"The selected model's weights could not be located." = "選択したモデルの重みが見つかりません。";
"Failed to load model for benchmark: %@" = "ベンチマーク用モデルの読み込みに失敗: %@";
"Benchmark generation failed: %@" = "ベンチマーク生成に失敗: %@";
"K Cache" = "Kキャッシュ";
"V Cache" = "Vキャッシュ";
"KV Offload" = "KVオフロード";
"On" = "オン";
"Off" = "オフ";
"GPU" = "GPU";
"CPU" = "CPU";
"%.1f tok/s" = "%.1f tok/s";
"%.1fs" = "%.1fs";
"%.2fs" = "%.2fs";
"Move Up" = "上に移動";
"Move Down" = "下に移動";
"Remove" = "削除";
"Startup remote options" = "起動時のリモートオプション";
"No models cached yet. Open the backend to refresh its catalog." = "まだモデルがキャッシュされていません。カタログを更新するにはバックエンドを開いてください。";
"We'll try this saved identifier even though it's not in the latest catalog." = "最新のカタログになくても、この保存した識別子を試します。";
"This backend is unavailable. Remove it or pick another option." = "このバックエンドは利用できません。削除するか別のオプションを選んでください。";
"Backend removed" = "バックエンドを削除しました";
"What is Max Chunks?" = "最大チャンクとは？";
"What is Similarity Threshold?" = "類似度しきい値とは？";
"Approx. %@" = "約%@";
"Advanced mode shows developer options and diagnostics." = "詳細モードでは開発者オプションと診断を表示します。";
"Simple mode hides advanced settings for a cleaner interface." = "シンプルモードでは高度な設定を隠し、すっきりしたUIにします。";
"Hide advanced controls" = "高度なコントロールを非表示";
"Show advanced controls" = "高度なコントロールを表示";
"Adjust model settings" = "モデル設定を調整";
"Model Settings" = "モデル設定";
"SLM models are not supported on this platform." = "このプラットフォームではSLMモデルはサポートされていません。";
"Model likely exceeds memory budget. Lower context or choose a smaller quant." = "モデルがメモリ予算を超えている可能性があります。コンテキストを下げるか小さい量子化を選んでください。";
"Apple bundle models aren't supported on macOS yet." = "AppleバンドルモデルはまだmacOSでサポートされていません。";
"Estimate: %@\nBudget: %@\nContext length: %@ tokens\n\nThis is an estimate based on your device’s memory budget, context length (KV cache), and typical runtime overheads. Actual usage may vary." = "推定: %@\n予算: %@\nコンテキスト長: %@ トークン\n\nこれはデバイスのメモリ予算、コンテキスト長（KVキャッシュ）、一般的なランタイムオーバーヘッドに基づく推定値です。実際の使用量は変動します。";
"Model likely fits in RAM" = "モデルはRAMに収まりそうです";
"Model may not fit in RAM" = "モデルがRAMに収まらない可能性があります";
"Fits in RAM (estimated)" = "RAMに収まる（推定）";
"May not fit (estimated)" = "収まらない可能性（推定）";
"Please provide a backend name." = "バックエンド名を入力してください。";
"A backend with this name already exists." = "この名前のバックエンドは既に存在します。";
"Backend not found." = "バックエンドが見つかりません。";
"This Noema Relay device is already configured." = "この Noema Relay デバイスはすでに設定されています。";
"OpenAI API" = "OpenAI API";
"LM Studio" = "LM Studio";
"Ollama" = "Ollama";
"Cloud Relay" = "Cloud Relay";
"Noema Relay" = "Noema Relay";
"Compatible with OpenAI-style /v1 endpoints" = "OpenAI 形式の /v1 エンドポイントと互換性があります";
"Connect to LM Studio's REST server" = "LM Studio の REST サーバーに接続";
"Target an Ollama host for chat and pulls" = "チャットと取得用に Ollama ホストを指定";
"Use Noema's Cloud Relay on macOS" = "macOS で Noema の Cloud Relay を使用";
"Pair with your Mac over CloudKit" = "CloudKit 経由で Mac とペアリング";
"Please provide the CloudKit container identifier." = "CloudKit コンテナIDを入力してください。";
"Please provide the host device ID from the Mac relay." = "Mac リレーのホストデバイスIDを入力してください。";
"Missing host device ID for Noema Relay." = "Noema Relay のホストデバイスIDがありません。";
"Missing CloudKit container identifier." = "CloudKit コンテナIDがありません。";
"Relay catalog unavailable." = "リレーのカタログを利用できません。";
"Relay catalog is still syncing. Open the Mac relay, ensure it is signed into iCloud, then try again in a moment." = "リレーのカタログを同期中です。Macのリレーを開き、iCloudにサインインしていることを確認してから再試行してください。";
"Terms of Use" = "利用規約";
"Privacy Policy" = "プライバシーポリシー";
"Contact Support" = "サポートに問い合わせる";
"Write a Review" = "レビューを書く";
"Notes & Issues" = "注意事項と問題";
"Qwen3-1.7B is a compact and efficient model from the Qwen3 family, suitable for on-device usage with strong general capabilities." = "Qwen3-1.7B は Qwen3 ファミリーのコンパクトで高効率なモデルで、強力な汎用性能を持ち端末上での利用に適しています。";
"Gemma 3n E2B is a lightweight instruction-tuned model from Google's Gemma family, optimized for efficient on-device conversations." = "Gemma 3n E2B は Google の Gemma ファミリーの軽量な命令調整モデルで、端末上での効率的な会話に最適化されています。";
"Gemma 3n E2B is an instruction-tuned variant of Google's Gemma family built for efficient reasoning on low-resource devices.\nAvailable in GGUF quants (Q3_K_M, Q4_K_M, Q6_K) and an MLX 4-bit build for Apple Silicon.\n" = "Gemma 3n E2B はリソースの限られたデバイスで効率的に推論するために作られた、Gemma ファミリーの指示特化バージョンです。\nGGUF 量子化 (Q3_K_M, Q4_K_M, Q6_K) と Apple Silicon 向けの MLX 4 ビット版が用意されています。\n";
"Phi-4 Mini Reasoning is a lightweight model from the Phi-4 family, tuned for strong reasoning and efficiency across tasks." = "Phi-4 Mini Reasoning は Phi-4 ファミリーの軽量モデルで、強力な推論と効率に調整されています。";
"Phi-4 Mini Reasoning — a compact model in Microsoft’s Phi-4 line designed for logical reasoning, problem solving, and instruction-following. \nDistributed in efficient GGUF quants (Q3_K_L, Q4_K_M, Q6_K) and an MLX 4-bit variant for Apple Silicon devices.\n" = "Phi-4 Mini Reasoning — Microsoft の Phi-4 系列のコンパクトモデルで、論理推論、問題解決、指示追従のために設計されています。\n効率的な GGUF 量子化 (Q3_K_L, Q4_K_M, Q6_K) と Apple Silicon 向け MLX 4 ビット版で提供されます。\n";
"Runtime Safety" = "実行時の安全性";
"Bypass RAM safety check (may cause crashes)" = "RAM 安全チェックを無視する（クラッシュの可能性あり）";
"Estimate for" = "推定対象";
"Off-grid Mode" = "オフグリッドモード";
"Delete All Chats" = "すべてのチャットを削除";
"Reset App Data" = "アプリデータをリセット";
"Max Chunks" = "最大チャンク";
"Delete Embedding Model" = "埋め込みモデルを削除";
"Override the app language. Defaults to the device language on first launch." = "アプリの言語を上書きします。初回起動時はデフォルトで端末の言語が使われます。";
"Swap between the new stacked chat panel and the classic tab bar layout." = "新しい積み重ねチャットパネルと従来のタブバー表示を切り替えます。";
"Available Quantizations" = "利用可能な量子化";
"Sort quantizations" = "量子化を並べ替え";
"Quant" = "量子化";
"Size ↑" = "サイズ ↑";
"Size ↓" = "サイズ ↓";
"Model Library" = "モデルライブラリ";
"Type to filter models…" = "モデルを絞り込むには入力…";
"No models match your search." = "検索に一致するモデルがありません。";
"Browse Explore tab" = "Explore タブを開く";
"Manually choose parameters" = "パラメータを手動で選択";
"The base URL looks invalid. Please include the host (e.g. http://127.0.0.1:1234)." = "ベースURLが無効のようです。ホストを含めてください (例: http://127.0.0.1:1234)。";
"Could not build the remote endpoint URL." = "リモートエンドポイントのURLを作成できませんでした。";
"The server returned an unexpected response." = "サーバーが予期しない応答を返しました。";
"Server responded with status code %d." = "サーバーのステータスコード: %d";
"Server responded with status code %d: %@" = "サーバー応答コード %1$d: %2$@";
"Failed to decode server response." = "サーバー応答のデコードに失敗しました。";

"Explore" = "探索";
"Search datasets" = "データセットを検索";
"Download" = "ダウンロード";
"Offline" = "オフライン";
"Tap to load" = "タップして読み込み";
"%d models" = "%d 件のモデル";
"Updated %@" = "%@に更新";
"No models fetched yet" = "まだモデルが取得されていません";
"Auth" = "認証";
"Local Network" = "ローカルネットワーク";
"Direct" = "直接";
"LAN" = "LAN";
"LAN · %@" = "LAN · %@";
"Backend" = "バックエンド";
"Base URL" = "ベースURL";
"Chat Path" = "チャットパス";
"Models Path" = "モデルパス";
"Endpoint Type" = "エンドポイントの種類";
"Endpoints" = "エンドポイント";
"Authentication" = "認証";
"Model Identifiers" = "モデル識別子";
"Name" = "名前";
"Host device ID" = "ホストデバイスID";
"Host Device ID" = "ホストデバイスID";
"CloudKit container identifier" = "CloudKit コンテナID";
"Field requirements will depend on your specific backend deployment." = "必須フィールドはバックエンドの構成によって異なります。";
"Uses Noema Relay configuration" = "Noema Relay の構成を使用";
"Chat: %@\nModels: %@" = "チャット: %@\nモデル: %@";

"Downloaded" = "ダウンロード済み";
"Compressed Text" = "圧縮テキスト";
"Small" = "小";
"Medium" = "中";
"Large" = "大";
"Very Large" = "特大";
"Extreme" = "極大";
"Under 10 MB" = "10MB 未満";
"10–50 MB" = "10〜50MB";
"50–200 MB" = "50〜200MB";
"200–500 MB" = "200〜500MB";
"Over 500 MB" = "500MB 以上";
"Estimated Embedding Time" = "推定埋め込み時間";
"Peak RAM Usage" = "最大RAM使用量";
"Dataset Size" = "データセットのサイズ";
"Performance Note" = "パフォーマンスの注意";
"Recommendation" = "おすすめ";
"Remember:" = "覚えておいてください:";
"• Close other applications to free up RAM" = "• RAMを解放するために他のアプリを閉じる";
"• Embedding happens locally on your device" = "• 埋め込みはデバイス上でローカルに実行されます";
"• Larger datasets take exponentially more time" = "• 大きいデータセットほど時間が指数的にかかります";
"• You can pause and resume downloads if needed" = "• 必要に応じてダウンロードを一時停止・再開できます";
"Dataset Requirements" = "データセットの要件";
"Got it" = "了解";
"Check Requirements" = "要件を確認";
"< 1 minute" = "1 分未満";
"%d minutes" = "%d 分";
"This dataset should embed quickly with minimal resource usage. Perfect for testing and quick experiments." = "このデータセットは最小限のリソースで素早く埋め込みできるはずです。テストや簡単な実験に最適です。";
"This dataset is a reasonable size for most systems. Embedding should complete in a few minutes." = "このデータセットはほとんどのシステムに適したサイズです。数分で埋め込みが完了するはずです。";
"This is a substantial dataset. Ensure you have adequate RAM and expect embedding to take 10–30 minutes." = "かなり大きいデータセットです。十分なRAMを確保し、10〜30分かかると見込んでください。";
"This is a very large dataset. Embedding may take 30–60 minutes and requires significant RAM." = "非常に大きいデータセットです。埋め込みには30〜60分かかり、多くのRAMを必要とします。";
"This is an extremely large dataset. Consider splitting it into smaller parts for better performance." = "極めて大きいデータセットです。パフォーマンス向上のために小さく分割することを検討してください。";
"Go ahead and download! This size works well on all systems." = "そのままダウンロードして大丈夫です！このサイズはどのシステムでもうまく動きます。";
"Recommended for most users. Make sure you have at least 4GB of free RAM." = "ほとんどのユーザーに推奨。少なくとも4GBの空きRAMを確保してください。";
"Recommended only if you have 8GB+ RAM available. Close other applications before embedding." = "8GB以上のRAMがある場合のみ推奨。埋め込み前に他のアプリを閉じてください。";
"Recommended only for systems with 16GB+ RAM. Consider processing during off-hours." = "16GB以上のRAMを備えたシステムにのみ推奨。可能なら時間外に処理してください。";
"Not recommended for typical systems. Consider finding a smaller version or subset of this dataset." = "一般的なシステムには非推奨です。より小さいバージョンや部分セットを検討してください。";
"Sample dataset" = "サンプルデータセット";
"Ready" = "準備完了";
"Open" = "開く";
"Download Dataset" = "データセットをダウンロード";
"No files listed for this dataset." = "このデータセットにはファイルが一覧表示されていません。";
"This dataset's files are not currently supported for document retrieval." = "このデータセットのファイルは現在ドキュメント検索に対応していません。";
"Supported formats: %@" = "対応フォーマット: %@";
"Try another dataset if these formats aren't available." = "これらのフォーマットが利用できない場合は別のデータセットをお試しください。";
"Found unsupported: %@ …" = "非対応を検出: %@ …";
"This textbook appears to be available only as a web page. Noema can't import it as a dataset." = "この教科書はウェブページとしてのみ提供されているようです。Noema はデータセットとしてインポートできません。";
"Download complete" = "ダウンロード完了";
"Downloading…" = "ダウンロード中…";
"No compatible files found for retrieval. Supported formats: %@" = "検索に使用できる互換ファイルが見つかりません。対応フォーマット: %@";
"No internet connection." = "インターネットに接続していません。";
"Request timed out. Please try again." = "リクエストがタイムアウトしました。もう一度お試しください。";
"Connection was lost. Please try again." = "接続が失われました。もう一度お試しください。";
"Unexpected error: %@" = "予期しないエラー: %@";

"Model doesn't support GPU offload" = "このモデルはGPUオフロードに対応していません";
"Loading model…" = "モデルを読み込み中…";
"Select a model to load" = "読み込むモデルを選択してください";
"Please wait" = "お待ちください";
"Models Library" = "モデルライブラリ";
"Sort" = "並べ替え";
"Recency" = "新しい順";
"Size" = "サイズ";
"Name" = "名前";
"Load Failed" = "読み込みに失敗しました";
"Don't show again" = "今後表示しない";

/* Noema Relay – pairing & dataset helpers */
"Model file missing (.gguf)" = "モデルファイルが見つかりません (.gguf)";
"Model path missing" = "モデルパスが見つかりません";
"Imported Dataset" = "インポートしたデータセット";
"Dataset name" = "データセット名";
"Keep this device near the Mac that's running Noema Relay to import its settings." = "Noema Relay を実行している Mac の近くにこのデバイスを置いて、その設定をインポートしてください。";
"Scanning for your Mac relay…" = "Mac リレーをスキャン中…";
"Ready to scan nearby relays" = "近くのリレーをスキャンする準備完了";
"Bluetooth access is required to pair with the Mac relay." = "Mac リレーとペアリングするには Bluetooth へのアクセスが必要です。";
"Stop Scanning" = "スキャンを停止";
"Start Scan" = "スキャンを開始";
"Connection verified. Relay details imported from this Mac." = "接続を確認しました。この Mac からリレーの詳細をインポートしました。";
"Signal strength unavailable" = "信号強度を取得できません";
"Very close" = "とても近い";
"Nearby" = "近く";
"Within one room" = "同じ部屋の中";
"Move closer for a stronger signal" = "より強い信号を得るには、もっと近づいてください。";
"This Mac" = "この Mac";

/* Accessibility announcements */
"Model loaded." = "モデルを読み込みました。";
"Prompt submitted." = "プロンプトを送信しました。";
"Generating response…" = "応答を生成中…";
"Response generated." = "応答が生成されました。";

/* Tabs & accessibility labels */
"Stored" = "保存済み";
"Web Search" = "Web 検索";
"Open Stored" = "保存済みを開く";
"Message input" = "メッセージ入力";
"What is Web Search button?" = "Web 検索ボタンとは何ですか？";

/* Mac chat quick-load menu */
"Open Model Library" = "モデルライブラリを開く";
"Favorites" = "お気に入り";
"Recent" = "最近";
